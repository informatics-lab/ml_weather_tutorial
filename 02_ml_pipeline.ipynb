{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f9ac977-3490-46bc-9fb5-3ab1b1c0fb7d",
   "metadata": {},
   "source": [
    "# 2. Building a machine learning pipeline\n",
    "\n",
    "## Overview \n",
    "Having explored the data and defined the problem, we are ready to build an initial pipeline, with choices informed by our data exploration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e680eb7-e443-4eba-a683-4f105c24eeed",
   "metadata": {},
   "source": [
    "### Prerequisites\n",
    "\n",
    "* Same as notebook 1 in this tutorial series, plus successful completion of notebook 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ccf53c-ef98-4d47-bded-249a5cbd6707",
   "metadata": {},
   "source": [
    "### Learning Outcomes \n",
    "* Understand the fundamental steps in a machine learning pipeline\n",
    "* Understand key terminology in describing the machine learning pipeline\n",
    "* Gain an initial understanding of how to choose appropriate components for each stage in the pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ebdf3e3-2b5d-4aa0-b2e7-e621410a950e",
   "metadata": {},
   "source": [
    "### Best Practices & Values\n",
    "\n",
    "* Link to Data Theme\n",
    "* Link to ethics Theme\n",
    "* link to ML lifecycle theme"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab40a7e-6681-4e39-9135-fafc7eb55e15",
   "metadata": {},
   "source": [
    "## Tutorial - Key Elements of a Machine Learning Pipeline\n",
    "\n",
    "Using a series of data science and machine learning and algorithms to go from input data to a series of predictions is usually referred to as a pipeline. In this notebook we will be exploring the key components of such a pipeline by constructing and training a machine learning algorithm with some input data.\n",
    "\n",
    "In this notebook we will look at 2 pipelines, one for a **supervised** **classification** problem, and the other for an **unsupervised** clustering problem.\n",
    "\n",
    "The steps we will go through are as follows:\n",
    "* *Data Loading & Cleaning* - Start by loading the data, and filtering out any data considered to be unsuitable for training and evaluation of machine learning algorithms. Selection of appropriate data is an important way in which domain expertise is vital in getting good results.\n",
    "* *Feature Engineering* - The first step is to prepare the data for presentation to the algorithm. Different ways of presenting the data will emphasise different features, and choosing the right features is important for getting good results. Knowledge of what features to represent based on domain knowledge is again very important.\n",
    "* *Train/test Split* - Before we train the algorithm, we need to split into *train* and *test* sets. This is to ensure out algorithm doesn't *overfit*, learning irrelevant details that are not representative of the whole space of possible data, but rather that it generalises well.\n",
    "* *Data Preparation* - The machine learning algorithm only sees numbers as numbers, with no inherent understanding of meaning or context. We need to ensure different features are scaled to be comparable, otherwise big numbers will be treated as more important by the algorithm, irrespective of what those numbers mean. Value are typically scaled to a range of `[0,1]` or, assuming a Gaussian distribution, to have `mean=0` and `std_dev=1`.\n",
    "* *Algorithm Setup* - Here we select the particular algorithm e.g. neural network, k-means clustering, as well as specify the *hyperparameters*. It is important to distinguish between *parameters* and *hyperparameters*. \n",
    "  * Parameters are the values that calculated by the training process.\n",
    "  * Hyperparameters are values specified in algorithm setup, which are not altered by training. These need to be fine-tuned using an additional outer training loop; this process is known as hyperparameter tuning.\n",
    "* *Algorithm Training* - Execute the algorithm to calculate the best parameters for the chosen ML algorithm to fit the supplied training data\n",
    "* *Inference* - Once we have an algorithm, we use it to produce predictions, for both the train and test sets.\n",
    "* *Evaluation* - We then compare the predictions of the trained algorithms to expected results. For supervised learning, this will be against the supplied target values. For unsupervised learning, we will explore the results and their usefulness much like in exploratory data analysis.\n",
    "* *Interpretability & Explainability*  -xxx\n",
    "* *Model Storage* - Model training can be an expensive process that we don't want to perform too often. Once we have a model that performs well, we save its state so it can be reloaded and used subsequently for inference on later problems.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea5d77e3-6029-4379-b215-0d2c5dc16ae8",
   "metadata": {},
   "source": [
    "### Best Practices and Values\n",
    "The topics covered and the examples demonstrated in this notebook related to the themes of the Met Office machine learning best practices and values in the following ways\n",
    "* Ethics - Our algorithm development need to be transparent and justifiable.\n",
    "* Data - We need to consider how we use the data, as well as which data will give us the best results. More data is not always better. Rather select data that is good quality and fits the requirements of the problem\n",
    "* ML Lifecycle - Ensure we have a reproducible workflow. Save the trained model in a way that can easily be reused for inference.\n",
    "* Interpretability and Explainability - Use a simpler algorithm where possible if this will help explain and interpret the results\n",
    "* ML Pitfalls - Ensure the loss function represents the real world requirements and impact of business need to get ML results that deliver value.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a9a267-b411-484c-9615-031203571be9",
   "metadata": {},
   "source": [
    "### Key Terms\n",
    "\n",
    "* supervised learning - training an algorithm to map from input to target data or labels.\n",
    "* unsupervised learning - training an algorithm to find structure in data that has no labels.\n",
    "* regression - An algorithm that predicts a continuous value.\n",
    "* classification - An algorithm that predicts from a set of discrete values.\n",
    "* metric - A measure of the performance of the ML algorithm.\n",
    "* parameter - A value in the algorithm that is determined by the training process e.g. neural network weights or decision tree thresholds.\n",
    "* hyperparameter - A value in the algorithm that is not determined by training and must be specified or tuned. e.g. number of hidden layers or max number of decision tree levels.\n",
    "* feature engineering - the process of creating input variables for the ML algorithm that will give desirable results.\n",
    "* training set - The subset of your data that you use for training your algorithm.\n",
    "* validation set - The subset of your data that you use for testing your trained algorithm and which informs subsequent development to improve results.\n",
    "* test set - The subset of your data that you set aside and do not use while developing the algorithm. Once the development process is finished, you check the final result with this subset of the data to check it truly generalises to unseen data.\n",
    "* inference - Calculating predictions from input data using a trained algorithm.\n",
    "\n",
    "More information on jargon\n",
    "* [Google Machine Learning Glossary](https://developers.google.com/machine-learning/glossary)\n",
    "* [ML Cheat-sheet](https://ml-cheatsheet.readthedocs.io/en/latest/glossary.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33fc8c17-9b64-441a-a4ae-2528c51c2283",
   "metadata": {},
   "source": [
    "## Problem 1: Supervised Classification - Falkland Islands Rotor Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a3806c-9b7d-4b7e-855b-e28f256e22d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c56d11e-912d-4046-b7cf-9f839a74fd93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import datetime\n",
    "import os\n",
    "import functools\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9384dc-476b-4a0c-95e2-4eeb5c9f8f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3a8c1f-aea8-4d9f-893d-d76d48ba97ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a21a65-8caf-421b-999a-e62a67ce4102",
   "metadata": {},
   "outputs": [],
   "source": [
    "import iris\n",
    "import iris.quickplot\n",
    "import iris.coord_categorisation\n",
    "import cartopy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88896762-086d-4c15-af7c-c3bbf2d45641",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import sklearn.neural_network\n",
    "import sklearn.preprocessing\n",
    "import sklearn.tree\n",
    "import sklearn.ensemble\n",
    "import sklearn.cluster\n",
    "import sklearn.decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af772ca5-cc08-4b40-875b-fdfe99140d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    falklands_data_dir = os.environ['OPMET_ROTORS_DATA_ROOT']\n",
    "except KeyError:\n",
    "    falklands_data_dir = '/project/informatics_lab/data_science_cop/ML_challenges/2021_opmet_challenge'\n",
    "falklands_data_dir = pathlib.Path(falklands_data_dir) /  'Rotors'\n",
    "print(falklands_data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4def7624-7333-432b-ba0a-0ad0a0d36afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "falklands_data_fname = 'new_training.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e852d339-a91d-44e2-8051-9fc1aa8e23f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "falklands_data_path = falklands_data_dir / falklands_data_fname\n",
    "falklands_data_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20fbc099-cd72-4dd6-ae32-5dfa67402a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "falklands_df = pandas.read_csv(falklands_data_path, header=0).loc[1:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21956cda-c18f-4930-9ae4-8fcc1d999463",
   "metadata": {},
   "outputs": [],
   "source": [
    "falklands_df = falklands_df.drop_duplicates(subset='DTG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab63e83-b076-43aa-819a-019d99fa60fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "falklands_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d715f5c4-de59-42e3-8b47-fcd67e97af94",
   "metadata": {},
   "outputs": [],
   "source": [
    "falklands_df['time'] = pandas.to_datetime(falklands_df['DTG'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b8a49f2-eb8e-4184-aac4-97f426a718e9",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e38f0d-e90d-4b3c-ac05-71ccb0b21969",
   "metadata": {},
   "source": [
    "Having loaded the data, we then do some preprocessing. This includes:\n",
    "* Specifying feature names.\n",
    "* Converting wind speed / direction back to u/v wind. This is because these parameters will vary more smoothly for northerly winds, which is the wind we are interested in.\n",
    "* Preparing the target variable, including filling in missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7bfd1d-158a-4663-a058-6f59bc0a881b",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_feature_names = [f'air_temp_{i1}' for i1 in range(1,23)]\n",
    "humidity_feature_names = [f'sh_{i1}' for i1 in range(1,23)]\n",
    "wind_direction_feature_names = [f'winddir_{i1}' for i1 in range(1,23)]\n",
    "wind_speed_feature_names = [f'windspd_{i1}' for i1 in range(1,23)]\n",
    "target_feature_name = 'rotors_present'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef7b166a-e066-4da5-b40b-5139eec457dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_names = [\n",
    "    'air_temp_obs',\n",
    "    'dewpoint_obs',\n",
    "    'wind_speed_obs',\n",
    "    'wind_direction_obs',\n",
    "]\n",
    "\n",
    "obs_feature_names = [\n",
    "    'air_temp_obs',\n",
    "    'dewpoint_obs',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d73fbf-1d43-4a04-864b-ff32161619f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_v_wind(wind_dir_name, wind_speed_name, row1):\n",
    "    return math.cos(math.radians(row1[wind_dir_name])) * row1[wind_speed_name]\n",
    "\n",
    "def get_u_wind(wind_dir_name, wind_speed_name, row1):\n",
    "    return math.sin(math.radians(row1[wind_dir_name])) * row1[wind_speed_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a632c6-89ef-46a1-9730-8a1f6c7e6e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "u_feature_template = 'u_wind_{level_ix}'\n",
    "v_feature_template = 'v_wind_{level_ix}'\n",
    "u_wind_feature_names = []\n",
    "v_wind_features_names = []\n",
    "for wsn1, wdn1 in zip(wind_speed_feature_names, wind_direction_feature_names):\n",
    "    level_ix = int( wsn1.split('_')[1])\n",
    "    u_feature = u_feature_template.format(level_ix=level_ix)\n",
    "    u_wind_feature_names += [u_feature]\n",
    "    falklands_df[u_feature] = falklands_df.apply(functools.partial(get_u_wind, wdn1, wsn1), axis='columns')\n",
    "    v_feature = v_feature_template.format(level_ix=level_ix)\n",
    "    v_wind_features_names += [v_feature]\n",
    "    falklands_df[v_feature] = falklands_df.apply(functools.partial(get_v_wind, wdn1, wsn1), axis='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9380ded-6669-450d-ac50-0ac74247ee2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "wdn1 = 'wind_direction_obs'\n",
    "wsn1 = 'wind_speed_obs'\n",
    "u_feature = u_feature_template.format(level_ix='obs')\n",
    "obs_feature_names += [u_feature]\n",
    "falklands_df[u_feature] = falklands_df.apply(functools.partial(get_u_wind, wdn1, wsn1), axis='columns')\n",
    "v_feature = v_feature_template.format(level_ix='obs')\n",
    "obs_feature_names += [v_feature]\n",
    "falklands_df[v_feature] = falklands_df.apply(functools.partial(get_v_wind, wdn1, wsn1), axis='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848ccb16-717b-492b-9efb-a2da6da62a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "falklands_df[obs_feature_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a411bc9-538e-4dda-9207-160e7e9aa63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "falklands_df['Rotors 1 is true']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14521bf0-1c64-4325-acbc-14cbe61402f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "falklands_df[target_feature_name] =  falklands_df['Rotors 1 is true']\n",
    "falklands_df.loc[falklands_df[falklands_df['Rotors 1 is true'].isna()].index, target_feature_name] = 0.0\n",
    "falklands_df[target_feature_name]  = falklands_df[target_feature_name] .astype(bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d34e3d4-5e76-4ada-9c54-623f72444b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "falklands_df[target_feature_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f5da24f-8047-4cca-b696-37b6afe6cd57",
   "metadata": {},
   "outputs": [],
   "source": [
    "falklands_df[target_feature_name].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2296a3-a8bd-4ca8-b9d7-915585324f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "falklands_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a286c7-6ba7-4b3c-b0a8-1d8ae84e3f6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4424f4d2-5ee6-462b-bdec-60da81b5c753",
   "metadata": {},
   "source": [
    "## Train/test Split\n",
    "\n",
    "Splitting data into train/validation/test sets\n",
    "\n",
    "To consider \n",
    "* consistency of distributions\n",
    "* class imbalance\n",
    "* correlation between samples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60098da2-8d5c-49a6-85d7-7d8bc7623f54",
   "metadata": {},
   "source": [
    "An initial option might be to split randomly, using the built-in scikit learn functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dbd1a79-51bd-40ce-95c5-633004f01097",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_fraction = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488b0290-d87c-46fa-884b-215d0bebbba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_random_df, test_random_df = sklearn.model_selection.train_test_split(falklands_df)\n",
    "train_random_df.shape, test_random_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e040aa78-80b4-455b-a7ee-41f82c98d5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_random_df[target_feature_name].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d16cbe98-430c-42d0-ab04-d8d0f0acf3df",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_random_df[target_feature_name].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7548195-49b8-4e20-802b-9423a526e94e",
   "metadata": {},
   "source": [
    "We know that our 2 classes (rotor detected/ no rotor detected) are imbalanced, so we might want to select from each class, to ensure our train/test splits have distributions which reflect the larger distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad10ddd-372e-4785-bb96-821cf4e5769b",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_no_rotors = sum(falklands_df[target_feature_name] == False)\n",
    "num_with_rotors = sum(falklands_df[target_feature_name] == True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e9b35d9-7176-407d-8d4c-deed6af86f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_no_rotors = falklands_df[falklands_df[target_feature_name] == False]\n",
    "data_with_rotors = falklands_df[falklands_df[target_feature_name] == True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a6cac1-8c2c-40bd-95af-1191abddf514",
   "metadata": {},
   "outputs": [],
   "source": [
    "falklands_df['test_set'] = False\n",
    "falklands_df.loc[list(data_no_rotors.sample(frac=test_fraction).index) + list(data_with_rotors.sample(frac=test_fraction).index),'test_set'] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa79b05a-acd9-4a76-81a9-295e6bd54fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_class_df = falklands_df[falklands_df['test_set'] == False]\n",
    "test_class_df = falklands_df[falklands_df['test_set'] == True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "503f53d9-e553-4eb4-982b-9389f5bc7537",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_class_df[target_feature_name].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d90b87f5-6ed2-4ad2-aaf0-6667749662d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_class_df[target_feature_name].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f7460ea-f3f0-47f4-a4e5-1318138d874f",
   "metadata": {},
   "source": [
    "We also know though that data points from adjacent points in time are likely to be correlated. As a result data in our test set will be correlated with that in our train set if we split randomly. Instead for this problem, we should split by time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7196ee19-b04b-4e87-ab27-f1c1cc696e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = falklands_df[falklands_df['time'] < datetime.datetime(2020,1,1,0,0)]\n",
    "test_df = falklands_df[falklands_df['time'] > datetime.datetime(2020,1,1,0,0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d94542-d781-4aa2-bb04-af8017bd20d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[target_feature_name].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251296a7-2fc5-4472-8786-0128229aa195",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df[target_feature_name].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95bee85d-0e9a-4bc0-bb8d-a371ee72e024",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56aeabfc-e039-4342-b42c-67b14e47da20",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_feature_names = temp_feature_names + humidity_feature_names + u_wind_feature_names + v_wind_features_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d63d4b-e984-4d0e-8d17-ced7884418cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "preproc_dict = {}\n",
    "for if1 in input_feature_names:\n",
    "    scaler1 = sklearn.preprocessing.StandardScaler()\n",
    "    scaler1.fit(train_df[[if1]])\n",
    "    preproc_dict[if1] = scaler1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897ff819-9a9b-41cc-90de-bd87607d2b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_encoder = sklearn.preprocessing.LabelEncoder()\n",
    "target_encoder.fit(train_df[[target_feature_name]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e779f9d6-bdb9-4376-a3f6-c71b426620a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preproc_input(data_subset, pp_dict):\n",
    "    return numpy.concatenate([scaler1.transform(data_subset[[if1]]) for if1,scaler1 in pp_dict.items()],axis=1)\n",
    "\n",
    "def preproc_target(data_subset, enc1):\n",
    "     return enc1.transform(data_subset[[target_feature_name]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef982738-d95a-4c66-a480-cfdabf600a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = preproc_input(train_df, preproc_dict)\n",
    "y_train = preproc_target(train_df, target_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57734f05-af8d-438c-b9c5-7cf24efcd533",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape, X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e94c53e5-df08-4f33-9eb3-1e0fa8c67238",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = preproc_input(test_df, preproc_dict)\n",
    "y_test = preproc_target(test_df, target_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f051cff4-8498-498c-a081-2c2bef755596",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4aa844a-d198-476e-afb6-7a6328dc7020",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_tuples = [\n",
    "    (X_train, y_train),\n",
    "    (X_test, y_test),    \n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a312cb28-8847-4934-85ee-36b81f2ad349",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Algorithm Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ed1fb1-901b-4229-a63f-6fb6e8d81354",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers_params = {\n",
    "    'decision_tree': {'class': sklearn.tree.DecisionTreeClassifier, 'opts': {'max_depth':10, 'class_weight':'balanced'}},\n",
    "    'random_forest': {'class': sklearn.ensemble.RandomForestClassifier, 'opts': {'max_depth':10, 'class_weight':'balanced'}},\n",
    "     'ann_5_500': {'class': sklearn.neural_network.MLPClassifier, 'opts': {'hidden_layer_sizes':(500,500,500,500,500)}},   \n",
    "     'ann_3_700': {'class': sklearn.neural_network.MLPClassifier, 'opts': {'hidden_layer_sizes':(700,700,700)}},   \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a03fd3-5a30-4307-8ccb-fb104f1a947b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "classifiers_dict = {}             \n",
    "for clf_name, clf_params in classifiers_params.items():\n",
    "    clf1 = clf_params['class'](**clf_params['opts'])\n",
    "    classifiers_dict[clf_name] = clf1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c231d98-4881-439b-bd0a-ab27a8b0fc5b",
   "metadata": {},
   "source": [
    "## Algorithm Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bcc70f8-8551-42ef-ad21-a8027f6df7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for clf_name, clf1 in classifiers_dict.items():\n",
    "    print(clf_name)\n",
    "    clf1.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610aff10-dee4-405c-b805-218fbe6cc4b9",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57661dd6-26da-49e5-8556-12b46baf74b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_train = {}\n",
    "\n",
    "for clf_name, clf1 in classifiers_dict.items():\n",
    "    y_pred = clf1.predict(X_train)\n",
    "    y_pred_train[clf_name] = y_pred\n",
    "    train_df[f'pred_{clf_name}'] = y_pred\n",
    "    rotors_freq = list(train_df[f'pred_{clf_name}'].value_counts())\n",
    "    print(f'predicted classes train set: {rotors_freq}')\n",
    "    actual_freq = list(train_df[target_feature_name].value_counts())\n",
    "    print(f'actual classes train set: {actual_freq}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0035f496-254b-429c-856d-1f4689794dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test = {}\n",
    "\n",
    "for clf_name, clf1 in classifiers_dict.items():\n",
    "    y_pred = clf1.predict(X_test)\n",
    "    y_pred_test[clf_name] = y_pred\n",
    "    test_df[f'pred_{clf_name}'] = y_pred\n",
    "    rotors_freq = list(test_df[f'pred_{clf_name}'].value_counts())\n",
    "    print(f'predicted classes test set: {rotors_freq}')\n",
    "    actual_freq = list(test_df[target_feature_name].value_counts())\n",
    "    print(f'actual classes test set: {actual_freq}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de953fb7-7d79-4422-a938-61a3a6de80cd",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "To determine whether the algorithm is performing well, we need to measure the performance in some way. For supervised learning, metrics are divided by the two key types of problem: regression and classification. Whichever sort of algorithm you're using, getting good results depends on measuring the right thing that best reflects the real world impact, business need or physical reality of the system being studied. Choosing the wrong metric might result in a trained algorithm that seemingly performs well in development, but isn't actually that useful in the real world."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4e1ee2-3c42-41fc-a9fb-fd32eba6b1aa",
   "metadata": {},
   "source": [
    "We're dealing with a classification problem here, so we will use classification relevant metrics.\n",
    "The obvious metric to use is accuracy i.e. how many predictions did we get right? As we will see, this usually does represent the way want the algorithm to perform. Obviously we want all the predictions to be correct, but generally, some will be wrong. Depending on the impact of incorrect predictions, we may wish penalise certain mistakes more or less. Different sorts of metrics allow us to do that.\n",
    "    \n",
    "A common starting point is precision and recall. This is frequently confusing, and the [wikipedia page](https://en.wikipedia.org/wiki/Precision_and_recall) has numerous useful explanations and diagrams. Precision is the portion of data points predicted as being in a class that are actually in that class. *Recall* in the proportion of data points tat are in a specific class in reality that are predicted as being in that class.\n",
    "![Precision and recall](images/Precisionrecall.svg)\n",
    "\n",
    "Graphic taken from Wikipedia\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92c2875-1425-479c-a03a-aefc278bd383",
   "metadata": {},
   "outputs": [],
   "source": [
    "prec, recall, f1, support = sklearn.metrics.precision_recall_fscore_support(\n",
    "    y_train,\n",
    "    classifiers_dict['decision_tree'].predict(X_train), \n",
    ")\n",
    "\n",
    "print(f'precision={prec},\\n recall={recall},\\n f1-score={f1},\\n support={support} ')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a1bc24-08ce-47ac-8a38-3cd9355cede9",
   "metadata": {},
   "source": [
    "Here we see that we have poor precision and good recall from decision tree. This means that we are predicting a high percentage of all actual rotor events, but we are also predicting as rotor events quite a lot of data points that are not actually rotor events.\n",
    "\n",
    "F1-score combines precision and recall to create a balanced metric. \n",
    "\n",
    "$F_{1}= 2 * \\frac{precision * recall}{precision + recall}$\n",
    "\n",
    "The One in the name reflects equal weighting of precision and recall in the metric. We can start to come up with more sophisticated metrics by giving more weight to one or the other, in the $F_{\\beta}$ metric\n",
    "\n",
    "$F_{\\beta} = (1 + \\beta)^2 * \\frac{precision * recall}{\\beta^2 *precision + recall}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43743655-58eb-4049-99ae-e869b69a1187",
   "metadata": {},
   "source": [
    "Another way to look at the results is by using a confusion matrix, also referenced in the wikipedia page above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7056bab-e71b-4f06-b7aa-ea4e9a3527c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_negatives, false_positives, false_negatives, true_positives = sklearn.metrics.confusion_matrix(\n",
    "    y_train,\n",
    "    classifiers_dict['decision_tree'].predict(X_train),\n",
    ").ravel()\n",
    "true_negatives, false_positives, false_negatives, true_positives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9512c2d4-9dfe-4e4b-ae00-eef9940ec89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm1 = sklearn.metrics.confusion_matrix(\n",
    "    y_train,\n",
    "    classifiers_dict['decision_tree'].predict(X_train),\n",
    ")\n",
    "cm1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4abf4ecb-a833-4581-9d90-08e8e834b2f1",
   "metadata": {},
   "source": [
    "The confusion matrix shows us where the algorithm correctly predicts Yes or No for a class, and where it gets it wrong.\n",
    "* top left = true negative - Model predicts no rotors where none were observed\n",
    "* top right = false positive (false alarm) - Model predicts rotors where no rotors were observed\n",
    "* bottom left = false negative (miss) - Model predicts no rotors where rotors were actually observed\n",
    "* bottom right = true positive (hit) - Model predicts rotors where rotors were observed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe6bc4c-fa5e-46d1-8220-8d7a83f6ecde",
   "metadata": {},
   "source": [
    "We can use this as the basis for other useful metrics. Hit rate is the proportion of data points where rotors were observed and where rotors were actually predicted by the algorithm. The higher the hit rate, the more trust there will be in any positive predictions of rotors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f571f67a-4759-47db-9804-f070dd51fd63",
   "metadata": {},
   "outputs": [],
   "source": [
    "hit_rate = cm1[1,1] / (cm1[1,1]+cm1[1,0])\n",
    "hit_rate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c6e39e-258a-4df5-a129-a0cf461634a1",
   "metadata": {},
   "source": [
    "False alarm rate is proportion of data points where no rotors were observed where the model incorrectly predicted rotors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8caf79c-2441-4af3-b632-b84086544501",
   "metadata": {},
   "outputs": [],
   "source": [
    "false_alarm_rate = cm1[0,1] / (cm1[0,1]+cm1[0,0])\n",
    "false_alarm_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af003541-7eb1-41fe-942f-505a00b5d062",
   "metadata": {},
   "source": [
    "The question then, is what do we really care about in this problem? What sort of errors are the most impactful? In our case, it is very important not to miss any rotor events, so false negatives are very bad and should be penalised. False positives erode trust in the forecast, but it is less costly to predict a few events that don't happen than to miss any events at all. So we might consider the False Alarm and the Miss rate to be very important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2860845f-af06-49ba-be2e-2835a7d00638",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics1 = []\n",
    "for X1, y1 in train_test_tuples:\n",
    "    md1 = {'classifier': [],\n",
    "           'precision_noRotor': [], 'precision_rotor': [],\n",
    "           'recall_noRotor': [], 'recall_rotor': [], \n",
    "           'f1_noRotor': [], 'f1_rotor': [], \n",
    "           'hit_rate': [], 'false_alarm_rate': []\n",
    "          }\n",
    "    for clf_name, clf1 in classifiers_dict.items():\n",
    "        md1['classifier'] += [clf_name]\n",
    "        prec, recall, f1, support = sklearn.metrics.precision_recall_fscore_support(clf1.predict(X1), y1)\n",
    "        md1['precision_noRotor'] += [prec[0]]\n",
    "        md1['precision_rotor'] += [prec[1]]\n",
    "        md1['recall_noRotor'] += [recall[0]]\n",
    "        md1['recall_rotor'] += [recall[1]]\n",
    "        md1['f1_noRotor'] += [f1[0]]\n",
    "        md1['f1_rotor'] += [f1[1]]\n",
    "        cm1 = sklearn.metrics.confusion_matrix(clf1.predict(X1), y1)\n",
    "        hit_rate = cm1[1,1] / (cm1[1,1]+cm1[1,0])\n",
    "        md1['hit_rate'] += [hit_rate]\n",
    "        false_alarm_rate = cm1[0,1] / (cm1[0,1]+cm1[0,0])\n",
    "        md1['false_alarm_rate'] += [false_alarm_rate]\n",
    "    metrics1 += [md1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf350938-f8b0-46f1-ab90-0c052d6a0214",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_metrics_df = pandas.DataFrame(metrics1[0])\n",
    "test_metrics_df = pandas.DataFrame(metrics1[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6bfd084-d63b-4393-94eb-18f54a9c854a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1 = matplotlib.pyplot.figure(figsize=(16,20))\n",
    "ax1 = fig1.add_subplot(3,2,1,title='precision (train) for rotors present class')\n",
    "train_metrics_df.plot.bar(x='classifier', y='precision_rotor', ax=ax1, ylim=[0,1])\n",
    "ax1 = fig1.add_subplot(3,2,2,title='recall (train) for rotors present class')\n",
    "train_metrics_df.plot.bar(x='classifier', y='recall_rotor', ax=ax1, ylim=[0,1])\n",
    "\n",
    "ax1 = fig1.add_subplot(3,2,3,title='precision (test) for rotors present class')\n",
    "test_metrics_df.plot.bar(x='classifier', y='precision_rotor', ax=ax1, ylim=[0,1])\n",
    "ax1 = fig1.add_subplot(3,2,4,title='recall (test) for rotors present class')\n",
    "test_metrics_df.plot.bar(x='classifier', y='recall_rotor', ax=ax1, ylim=[0,1])\n",
    "\n",
    "ax1 = fig1.add_subplot(3,2,5,title='false alarm rate (train) for rotors present class')\n",
    "train_metrics_df.plot.bar(x='classifier', y='false_alarm_rate', ax=ax1, ylim=[0,1])\n",
    "ax1 = fig1.add_subplot(3,2,6,title='false alarm rate (test) for rotors present class')\n",
    "test_metrics_df.plot.bar(x='classifier', y='false_alarm_rate', ax=ax1, ylim=[0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87325cef-e3cf-4e6b-b9be-9e295adeec8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_sedi(conf_mat):\n",
    "    hr1 = conf_mat[1,1] / (conf_mat[1,0] + conf_mat[1,1])\n",
    "    fa1 = conf_mat[0,1] / (conf_mat[0,0] + conf_mat[0,1])\n",
    "    sedi_score1 = (\n",
    "        (numpy.log(fa1) - numpy.log(hr1) - numpy.log(1.0-fa1) + numpy.log(1.0-hr1) )\n",
    "        / (numpy.log(fa1) + numpy.log(hr1) + numpy.log(1.0 - fa1) + numpy.log(1.0-hr1) )  )\n",
    "    return sedi_score1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d396441-4dea-4762-96b6-278931aa13d8",
   "metadata": {},
   "source": [
    "How though do we actually go about getting better results? One way is to understand that some algorithms output a pseudo-probability, and the classification is based on thresholding that value. By altering the threshold we get different predictions and thus different metric results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed485e2-9000-4f2c-a249-145daca24031",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred_raw = classifiers_dict['random_forest'].predict_proba(X_train)[:,1]\n",
    "y_test_pred_raw = classifiers_dict['random_forest'].predict_proba(X_test)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a6ed6b0-24c8-4821-aed9-c72d563a4d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds_list = list(numpy.arange(1e-3,0.995,5e-3))\n",
    "hit_rates = []\n",
    "hit_rates_test = []\n",
    "false_alarm_rates = []\n",
    "false_alarm_rates_test = []\n",
    "sedi_list = []\n",
    "sedi_list_test = []\n",
    "for threshold in thresholds_list:\n",
    "    y_pred_train = list(map(float, y_train_pred_raw > threshold)) \n",
    "    cm1 = sklearn.metrics.confusion_matrix(y_train, y_pred_train)\n",
    "    hit_rates += [cm1[1,1] / (cm1[1,0] + cm1[1,1])]\n",
    "    false_alarm_rates += [cm1[0,1] / (cm1[0,0] + cm1[0,1])]\n",
    "    sedi_list += [calc_sedi(cm1)]\n",
    "\n",
    "    y_pred_test = list(map(float, y_test_pred_raw > threshold)) \n",
    "    cm1 = sklearn.metrics.confusion_matrix(y_test, y_pred_test)\n",
    "    hit_rates_test += [cm1[1,1] / (cm1[1,0] + cm1[1,1])]\n",
    "    false_alarm_rates_test += [cm1[0,1] / (cm1[0,0] + cm1[0,1])]\n",
    "    sedi_list_test += [calc_sedi(cm1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a8891c6-6d8d-414d-9298-bb2fe79578d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = matplotlib.pyplot.figure(figsize=(16,10))\n",
    "ax1 = f1.add_subplot(1,2,1,title='hit rates vs false alarms (training data)')\n",
    "ax1.plot(thresholds_list, hit_rates,'r')\n",
    "ax1.plot(thresholds_list, false_alarm_rates,'b')\n",
    "ax1.plot(thresholds_list, sedi_list,'k')\n",
    "\n",
    "ax1 = f1.add_subplot(1,2,2,title='hit rates vs false alarms (test data)')\n",
    "ax1.plot(thresholds_list, hit_rates_test,'r')\n",
    "ax1.plot(thresholds_list, false_alarm_rates_test,'b')\n",
    "ax1.plot(thresholds_list, sedi_list_test,'k')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b34b80b3-24b0-47bf-9ae0-439dcc6b63dd",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Model Storage\n",
    "Once we have trained a model, we usually want to save the result for subsequent use in scientific work. There are a variety of ways of doing this. The most simple way in scikit learn is to pickle the data.\n",
    "\n",
    "discuss what elements need to be stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb8e95f8-d37c-42ab-8c4c-62495ff88094",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import tempfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1876b99-f31f-460f-8bd3-3799294cc6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_str = pickle.dumps(classifiers_dict['random_forest'])\n",
    "rf_str[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d31056-c3a5-43eb-ba6f-52fe22e4341d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tempfile.TemporaryDirectory() as td1:\n",
    "    with open(td1 / 'my_trained_classifier.pkl','wb') as clf_file_out1:\n",
    "        clf_file_out1.write(rf_str)\n",
    "    with open(td1 / 'my_trained_classifier.pkl','rb') as clf_file_in1:\n",
    "        train_clf_str1 = clf_file_in1.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e79847-141a-42f5-808c-af2d97857a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_rf_clf = pickle.loads(train_clf_str1)\n",
    "loaded_rf_clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e385e43b-7f5d-4b6f-8dcd-fd7801be6cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_rf_clf.predict(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c79faf-c9b6-4218-922d-3ceee5277ce1",
   "metadata": {},
   "source": [
    "There are more sophisticated ways to do this.\n",
    "* [TensorFlow](https://www.tensorflow.org/guide/keras/save_and_serialize) has a more robust custom format for saving and loading models\n",
    "* [ONNX](https://onnx.ai/) - A library to save models and perform inference\n",
    "* [ML Flow Models](https://www.mlflow.org/docs/latest/models.html) - Another library that can save models and do inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ac6f5c-8504-47b0-a8c3-f3e9c0eab525",
   "metadata": {},
   "source": [
    "## Problem 2 - Clustering weather types from ERA5\n",
    "\n",
    "So far we have looked an example of *supervised learning*, where we want to predict some target values from input values, which is what we call *labelled data*. Labelling is an expensive process, and where possible it is very useful to be able to find structure and patterns in unlabelled data. This can be particularly helpful to explore the data and find structures, as well as providing an approach to perform useful operations without the expense of labelling data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "181333cc-bbfe-4976-948b-9429882b7333",
   "metadata": {},
   "source": [
    "### Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8925fa9c-e48d-403d-9f7f-6c0e2be6dc90",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    era5_root = os.environ['ERA5_DATA_ROOT']\n",
    "except KeyError:\n",
    "    era5_root = '/project/informatics_lab/data_science_cop/era5/'\n",
    "era5_data_dir = pathlib.Path(era5_root) \n",
    "print(era5_data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de34937e-fc41-4e7f-b4b0-91b25d62a326",
   "metadata": {},
   "outputs": [],
   "source": [
    "era5_mslp_path = era5_data_dir / 'era5_mslp_UK_2017_2020.nc'\n",
    "print(era5_mslp_path)\n",
    "era5_mslp_path.is_file()\n",
    "# load era5 pressure data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a1303b-698a-4e50-8576-0d8abaa1f964",
   "metadata": {},
   "outputs": [],
   "source": [
    "era5_mslp_cube = iris.load_cube(str(era5_mslp_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13410ade-518f-4a6f-9fee-54a5ad3ecb3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.coord_categorisation.add_season_number(era5_mslp_cube, 'time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "865f1402-a075-4e33-a52e-21a8fba4eb4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "era5_mslp_cube"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad6a2e5-e655-4e3c-a287-ada60354c4d5",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Preprocessing features\n",
    "\n",
    "In this case, we previously did some preprocessing from the raw ERA5 data to prepare for our purposes. So far we:\n",
    "* Load MSLP data (hourly, global)\n",
    "* Extract UK area from global ERA5 data\n",
    "\n",
    "Next up we want to remove the seasonal averages, so that we can get season independent clusters of weather type which will be applicable all year round.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "930757dc-5211-45de-81f2-f4f8ab5bf243",
   "metadata": {},
   "outputs": [],
   "source": [
    "season_average = [era5_mslp_cube.extract(iris.Constraint(season_number=sn1)).collapsed(['time'],iris.analysis.MEAN).data for sn1 in range(4)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188fb847-a244-486f-af9a-5edbefcb2217",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "era5_flat_deseasoned = numpy.concatenate(\n",
    "    [(era5_mslp_cube.extract(iris.Constraint(season_number=sn1)).data - season_average[sn1]).reshape(\n",
    "    (-1, era5_mslp_cube.shape[1] * era5_mslp_cube.shape[2])) for sn1 in range(4)],\n",
    "    axis=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f877566-1e59-49a0-83e9-e4d1112e1dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "era5_mslp_cube.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24837a09-878b-432e-b956-79135eb4cb57",
   "metadata": {},
   "outputs": [],
   "source": [
    "era5_flat_deseasoned.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6230499-5954-4050-a7a6-a9db62efce8b",
   "metadata": {},
   "source": [
    "### Setup and train the algorithm\n",
    "Now that we have our data, we want to do some clustering. Scikit learn gives a [good overview to clustering algorithms](https://scikit-learn.org/stable/modules/clustering.html), though this covers only what is available in Sklearn; many other more advanced algorithms and implementations are available.\n",
    "\n",
    "We will use the basic [K-means algorithm](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html#sklearn.cluster.KMeans). This algorithm works as follow:\n",
    "1. randomly select n points, where n is the desired number of clusters (a hyperparameter)\n",
    "2. initiate clusters with each of the selected points the centre of a cluster\n",
    "3. for each point in the dataset, find the closest cluster centre and assign to that cluster\n",
    "4. for each of the clusters, find the centroid of all the points assigned to that cluster (or the nearest actual point to the centroid).\n",
    "5. assign those points as the new cluster centres, and repeat from point 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c3657a-7d76-425f-a465-f9670f4548f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_clusters = 10\n",
    "max_iter=1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb500fb-1154-4eb6-b583-846cab10d9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = sklearn.cluster.KMeans(n_clusters=num_clusters, random_state=0, max_iter=max_iter)\n",
    "kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d7e195-f26f-40ea-b6d1-e404e0dd5e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "kmeans.fit(era5_flat_deseasoned)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ce65ba-4a83-449b-acc4-78f56f90dcc9",
   "metadata": {},
   "source": [
    "### Inference and Evaluation\n",
    "In this case, we don't have a target to compare our results to\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f596317c-670d-4255-aaea-258b9f730bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_coord = iris.coords.DimCoord(list(range(10)),\n",
    "                                     var_name='cluster_number')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de77c866-1b2c-4850-80f9-51d7600ddc8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_centres = iris.cube.Cube(\n",
    "    data=kmeans.cluster_centers_.reshape((10,era5_mslp_cube.shape[1],era5_mslp_cube.shape[2],)),\n",
    "    dim_coords_and_dims = [(cluster_coord, 0), (era5_mslp_cube.coord('latitude'),1), (era5_mslp_cube.coord('longitude'),2)],\n",
    "    units=era5_mslp_cube.units,\n",
    "    var_name='cluster_mean_sea_level_pressure',\n",
    ")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8f42c0-a5bd-4a2d-90b3-16c159ab2089",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1 = matplotlib.pyplot.figure(figsize=(16,80))\n",
    "for ix1 in range(num_clusters):\n",
    "    ax1 = fig1.add_subplot(num_clusters,1,ix1+1,projection=cartopy.crs.PlateCarree())\n",
    "    iris.quickplot.contourf(cluster_centres[ix1,:,:],axes=ax1)\n",
    "    ax1.coastlines()    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d4ec97-1051-4998-987f-0809b796bc9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "season_clusters = [kmeans.predict(\n",
    "    (era5_mslp_cube.extract(iris.Constraint(season_number=sn1)).data - season_average[sn1]).reshape(\n",
    "    (-1, era5_mslp_cube.shape[1] * era5_mslp_cube.shape[2])) \n",
    ") for sn1 in range(4)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b9d148-3350-45c9-b4e0-b3db045a4dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "season_names =['dfj (nh winter)', 'mam (nh spring)', 'jja (nh summer)','son (nh autumn)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23f2f86-9951-4b69-80e2-9de1ea3e64d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1 = matplotlib.pyplot.figure(figsize=(16,16))\n",
    "for sn1, current_season in enumerate(season_clusters):\n",
    "    ax1 = fig1.add_subplot(2,2,sn1+1, title=season_names[sn1])\n",
    "    pandas.Series(current_season).plot.hist(ax=ax1)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec98fa0-2c3d-41cb-921c-558ff2e0d3bb",
   "metadata": {},
   "source": [
    "### Another approach - dimensionality reduction\n",
    "\n",
    "There are various ways of improving cluster results:\n",
    "* change how to describe clusters\n",
    "* change how to assign cluster membership\n",
    "  * change the distance metric between points\n",
    "* transform the points into a different space, which may preserve some properties and change others, hopefully to improve separation.  \n",
    "\n",
    "We will demonstrate this briefly by performing *principal component analysis* or *PCA*  on the data to reduce the dimensionality. This will also speed up calculation.\n",
    "\n",
    "Scikit Learn reference:\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8313b982-5d2c-4591-baac-9f5f83342624",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "pca_mslp = sklearn.decomposition.PCA(n_components=50, svd_solver='full')\n",
    "pca_mslp.fit(era5_flat_deseasoned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "073fad27-e818-4726-95b8-5c930e1d0a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "era5_dim_reduced = pca_mslp.transform(era5_flat_deseasoned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe78ec65-ec7b-4a38-8eac-9c98691f81b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "kmeans_mslp_reduced = sklearn.cluster.KMeans(n_clusters=num_clusters, random_state=0, max_iter=max_iter)\n",
    "kmeans_mslp_reduced.fit(era5_dim_reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ebf7f88-dcee-4008-8666-62c44c7e2280",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_centres_pca = iris.cube.Cube(\n",
    "    data=(pca_mslp.inverse_transform(kmeans_mslp_reduced.cluster_centers_)).reshape((10,era5_mslp_cube.shape[1],era5_mslp_cube.shape[2],)),\n",
    "    dim_coords_and_dims = [(cluster_coord, 0), (era5_mslp_cube.coord('latitude'),1), (era5_mslp_cube.coord('longitude'),2)],\n",
    "    units=era5_mslp_cube.units,\n",
    "    var_name='cluster_mean_sea_level_pressure',\n",
    ")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f71bd7c5-a7d8-4acf-84a1-21adbdbfb6f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1 = matplotlib.pyplot.figure(figsize=(16,80))\n",
    "for ix1 in range(num_clusters):\n",
    "    ax1 = fig1.add_subplot(num_clusters,1,ix1+1,projection=cartopy.crs.PlateCarree())\n",
    "    iris.quickplot.contourf(cluster_centres_pca[ix1,:,:],axes=ax1)\n",
    "    ax1.coastlines()    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c3107ab-0529-47da-8d13-be6b271a38a5",
   "metadata": {},
   "source": [
    "Now to calculate the clusters for each point we perform the following:\n",
    "* remove the seasonal mean\n",
    "* reshape the data\n",
    "* apply PCA transform to reduce dimensionality\n",
    "* predict which cluster each point belongs to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d64e49cd-16b9-4db9-9552-3bd4d375882d",
   "metadata": {},
   "outputs": [],
   "source": [
    "season_clusters_pca = [kmeans_mslp_reduced.predict( pca_mslp.transform(\n",
    "    (era5_mslp_cube.extract(iris.Constraint(season_number=sn1)).data - season_average[sn1]).reshape(\n",
    "    (-1, era5_mslp_cube.shape[1] * era5_mslp_cube.shape[2]))) \n",
    ") for sn1 in range(4)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501b0de1-bd01-4d73-a3a4-05f4cb684b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1 = matplotlib.pyplot.figure(figsize=(16,16))\n",
    "for sn1, current_season in enumerate(season_clusters_pca):\n",
    "    ax1 = fig1.add_subplot(2,2,sn1+1, title=season_names[sn1])\n",
    "    pandas.Series(current_season).plot.hist(ax=ax1)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7089a67-dde5-4531-918c-fd58904002aa",
   "metadata": {},
   "source": [
    "### Saving the model\n",
    "\n",
    "Here we could very easily save the data at key parts of the steps, then reload them to predict clusters on other data using our training outputs. In our case the key data are:\n",
    "* The seasonal means\n",
    "* The cluster centres\n",
    "* The PCA transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c890e74a-9009-4cc9-8237-01225a3d004e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_mslp.singular_values_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ad8947-6371-4622-98cf-ee208c91bdb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_mslp.get_covariance().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c09945-622d-4ca9-a038-3d3722376263",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Autoencoder and Latent Space K-Means\n",
    "\n",
    "**UNDER CONSTRUCTION**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f1ab59-aa1f-4486-9e04-dd8bf9038492",
   "metadata": {},
   "source": [
    "Exploring"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98329ef7-c77f-4838-9faa-562127517244",
   "metadata": {},
   "source": [
    "## Problem 3 - Radiation Emulation (Regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b00455-7968-455a-ab0f-1165a9fcd38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load radition dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80439fbf-3ee0-4bc9-bf27-8d7da5d3a45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare featureds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb5deb36-ff5c-4e99-b2d2-425a9d080670",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into train test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74648132-2e7d-45ab-93fc-e1651feae138",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up 1D CNN architectureb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "698096c2-ec82-4268-9aea-6be87a3efcd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7de817-c9c5-466e-8269-aa65afdfaaee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate appropriate metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f70d63-f4b4-44b3-8265-46ebdebecf73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "df1260a0-d80a-4566-a9bc-e9867904b6be",
   "metadata": {},
   "source": [
    "## Examples of use\n",
    "* You can see more example notebook relating to this challenge on the [Data Science CoP GitHub repository](https://github.com/MetOffice/data_science_cop/tree/master/challenges/2021_falklands_rotors).\n",
    "\n",
    "More Information on Weather Patterns can be found here, which the full much more sophisticated technique used in the real application:\n",
    "* [Met Office Info Page on Weather Types](https://www.metoffice.gov.uk/research/news/2016/new-weather-patterns-for-uk-and-europe)\n",
    "* [Paper by Neal, Fereday et al.](https://rmets.onlinelibrary.wiley.com/doi/full/10.1002/met.1563)\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df3187a-261f-4bed-ae13-9b11f47a5f77",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Next steps\n",
    "\n",
    "* [Rotors Challenge Notebooks](https://github.com/MetOffice/data_science_cop/tree/master/challenges/2021_falklands_rotors) \n",
    "* [Leeds University Notebooks](https://cemac.github.io/LIFD_ENV_ML_NOTEBOOKS/) \n",
    "* [Kaggle Weather Types Clustering Competition](https://www.kaggle.com/code/prakharrathi25/weather-data-clustering-using-k-means/notebook)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd25858d-bdc0-442a-98fe-bbad7a01b279",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Dataset Info\n",
    "\n",
    "### Falklands Rotors Challenge Dataset\n",
    "Crown Copyright 2021 - This dataset was created by Met Office Chief Operational Meteorologist Steve Ramsdale from Met Office forecast and observation data.\n",
    "* Model Data - Met Office Global 10km resolution model\n",
    "* Observations - made by meteorologists at Mount Pleasant airfield in the Falkland Islands.\n",
    "\n",
    "### ERA5\n",
    "ERA5 is Reanalysis data created by ECMWF. Reanalysis combines observations from many sources. by assimilating these into a forecast model (ECMWF's IFS in this case), to provide a consistent physically valid gridded dataset that is a close to observations as possible\n",
    "https://www.ecmwf.int/en/forecasts/datasets/reanalysis-datasets/era5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e69e18e-5fe4-41c6-8380-52719b9f082d",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "The format of this notebook is based on the [template for tutorial notebooks](https://github.com/geo-yrao/notebook-dev/blob/main/templates/NCAI_Training_Notebook_template%20-%20Distribution%20Copy.ipynb) developed by NOAA, available on GitHub."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
