{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f9ac977-3490-46bc-9fb5-3ab1b1c0fb7d",
   "metadata": {},
   "source": [
    "# 2. Building a machine learning pipeline\n",
    "\n",
    "## Overview \n",
    "Having explored the data and defined the problem, we are ready to build an initial pipeline, with choices informed by our data exploration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e680eb7-e443-4eba-a683-4f105c24eeed",
   "metadata": {},
   "source": [
    "### Prerequisites\n",
    "\n",
    "* Same as notebook 1 in this tutorial series, plus successful completion of notebook 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ccf53c-ef98-4d47-bded-249a5cbd6707",
   "metadata": {},
   "source": [
    "### Learning Outcomes \n",
    "* Understand the fundamental steps in a machine learning pipeline\n",
    "* Understand key terminology in describing the machine learning pipeline\n",
    "* Initial understanding of how to choose appropriate components for each stage in the pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ebdf3e3-2b5d-4aa0-b2e7-e621410a950e",
   "metadata": {},
   "source": [
    "### Best Practices & Values\n",
    "\n",
    "Link to Data Theme\n",
    "Link to ethics Theme\n",
    "link to ML lifecycle theme"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab40a7e-6681-4e39-9135-fafc7eb55e15",
   "metadata": {},
   "source": [
    "## Tutorial - Key Elements of a Machine Learning Pipeline\n",
    "\n",
    "Using a series of data science and machine learning and algorithms to go from input data to a series of predictions is usually referred to as a pipeline.  In this noteboook we will be exploring the key components of such a pipeline in constructing and training a machine learning algorithm with some input data.\n",
    "\n",
    "In this notebook we will look at 2 pipeline, one for a **supervised** **classification** problem, and the other for an **unsupervised** clustering problem.\n",
    "\n",
    "The steps we willl go through are as follows:\n",
    "* *Data Loading & Cleaning* - Start by loading the data, and filtering out any data considered to be unsuitable training and evaluation of machine learning algorithms. Selection of appropriate data is an important way in which domain expertise in vital in getting good results.\n",
    "* *Feauture Engineering* - The first step is to prepare the data for presenting to the algorithm. Different ways of presenting the data will emphasise different features, and choosing the right features is important for getting good results. Knowledge of what features represent based on domain knowledge is again very important.\n",
    "* *Train/test Split* - Before we train the algorithm, we need to split into *train* and *test* sets. This is to ensure out algorithm doesn't *overfit*, learning irrelevant details that are not representative of the whole space of possible data, but rather that in generalises well.\n",
    "* *Data Preparation* - The machine learning algorithm only sees numbers as numbers, with no inherent understnading oif meaning or context. We need to ensure different features are scaled to be comparable, otherwise big numbers will be treated as more important by the algorithm, irrespective of what those numbers mean. Value are typically scaled to a range of `[0,1]` or, assuming a gaussian distribution, to have `mean=0` and `std_dev=1`.\n",
    "* *Algorithm Setup* - Here we select the particular algorithm e,.g. neural network, k-means clustering, and specify the *hyperparameters*. It is important to distinguish between *parameters* and *hyperparameters*. \n",
    "  * Parameters are the values that calculated by the training process. \n",
    "  * Hyperparameters are values specified in algorithm setup, which are not altered by training. These need to be fine-tuned using an additional outer training loop called hyperparameter tuning.\n",
    "* *Algorithm Training* - Execute the algorithm to calculate the best parameters for the chosen ML algorithm to fit the supplied training data\n",
    "* *Inference* - Once we have an algorithm, we use it to produce predictions, for both the train and test sets.\n",
    "* *Evaluation* - We then compare the predictions of the trained algorithms to expected results. For supervised learning, this will be supplied target values. For unsupervised learning, we will expplore the results and their usefulness much like in exploratory data analysis.\n",
    "* *Interpretability & Explainability*  -xxx\n",
    "* *Model Storage* - Model training can be an expensive process that we don't want to perform too often, and. once we have a model that performs well we save its state so it can be reloaded and used subsequently for inference on later problems.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea5d77e3-6029-4379-b215-0d2c5dc16ae8",
   "metadata": {},
   "source": [
    "### Best Practices and Values\n",
    "The topics covered and the examples demonstrated in this notebook related to the themes of the Met Office machine learning best practices and values in the following ways\n",
    "* Ethics - Our algorithm development need to be transparent and justifiable.\n",
    "* Data - We need to consider how we use the data, as well as which data will give us the best results. More data is not always better. Rather select data that is good quality and fits the requirements of the problem\n",
    "* ML Lifecycle - Ensure we have a reproducible workflow. Save the trained model in a way that can easily bbe reused for inference.\n",
    "* Interpretability and Explainability - Use a simpler algorithm where possible if this will help exlain and interpret the results\n",
    "* ML Pitfalls - Ensure the loss function represents the real world requirements and impact of business need to get ML results that deliver value.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a9a267-b411-484c-9615-031203571be9",
   "metadata": {},
   "source": [
    "### Key Terms\n",
    "\n",
    "* supervised learning - training an algorithm to map from input to target data or labels.\n",
    "* unsupervised learning - training an algorithm to find structure in data that has no labels.\n",
    "* regression - An algorithm that predicts a continuous values.\n",
    "* classification - An algorithm that predicts from a set of discrete values\n",
    "* metric - A measure of the performance of the ML algorithm.\n",
    "* parameter - A value in the algorithm that is determined by the training process e.g. neural network weights or decision tree thresholds.\n",
    "* hyperparameter - A value in the algorithm that is not determined by training and must be specified or tuned. e.g. number of hidden layers or max number of decision tree levels.\n",
    "* feature engineering - the process of creating input variables for the ML aglorithm that will give desirable results.\n",
    "* training set - The subset of your data that you use for training your algorithm.\n",
    "* validation set - The subset of your data that you use for testing your trained algorithm and which informs subsequent development to improve results.\n",
    "* test set - The subset of your data that you set aside and do not use while developing the algorithm. Once the development process is finished, you check the final result with this subset of the data to check it truly generalises to unseen data.\n",
    "* inference - Calculating predictions from input data used a trained algorithm.\n",
    "\n",
    "More information on jargon\n",
    "* [Google Machine Learning Glossary](https://developers.google.com/machine-learning/glossary)\n",
    "* [ML Cheatsheet](https://ml-cheatsheet.readthedocs.io/en/latest/glossary.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33fc8c17-9b64-441a-a4ae-2528c51c2283",
   "metadata": {},
   "source": [
    "## Problem 1: Supervised Classification - Falkland Islands Rotor Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97a3806c-9b7d-4b7e-855b-e28f256e22d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c56d11e-912d-4046-b7cf-9f839a74fd93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import datetime\n",
    "import os\n",
    "import functools\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e9384dc-476b-4a0c-95e2-4eeb5c9f8f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd3a8c1f-aea8-4d9f-893d-d76d48ba97ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "54a21a65-8caf-421b-999a-e62a67ce4102",
   "metadata": {},
   "outputs": [],
   "source": [
    "import iris\n",
    "import iris.quickplot\n",
    "import iris.coord_categorisation\n",
    "import cartopy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "88896762-086d-4c15-af7c-c3bbf2d45641",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import sklearn.neural_network\n",
    "import sklearn.preprocessing\n",
    "import sklearn.tree\n",
    "import sklearn.ensemble\n",
    "import sklearn.cluster\n",
    "import sklearn.decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3416eb09-73fd-43b0-bb41-60a821bd7336",
   "metadata": {},
   "source": [
    "## Load Rotors Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "af772ca5-cc08-4b40-875b-fdfe99140d31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/project/informatics_lab/data_science_cop/ML_challenges/2021_opmet_challenge/Rotors\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    falklands_data_dir = os.environ['ML_TUTORIAL_DIR']\n",
    "except KeyError:\n",
    "    falklands_data_dir = '/project/informatics_lab/data_science_cop/ML_challenges/2021_opmet_challenge/Rotors'\n",
    "falklands_data_dir = pathlib.Path(falklands_data_dir) \n",
    "print(falklands_data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ec6fc9d6-95b4-47d2-995c-2c47e8ac3aca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PosixPath('/project/informatics_lab/data_science_cop/ML_challenges/2021_opmet_challenge/Rotors')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(falklands_data_dir.is_dir())\n",
    "falklands_data_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4def7624-7333-432b-ba0a-0ad0a0d36afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "falklands_data_fname = '2021_met_office_aviation_rotors.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5a7bfd1d-158a-4663-a058-6f59bc0a881b",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_feature_names = [f'air_temp_{i1}' for i1 in range(1,23)]\n",
    "humidity_feature_names = [f'sh_{i1}' for i1 in range(1,23)]\n",
    "wind_direction_feature_names = [f'winddir_{i1}' for i1 in range(1,23)]\n",
    "wind_speed_feature_names = [f'windspd_{i1}' for i1 in range(1,23)]\n",
    "target_feature_name = 'rotors_present'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e852d339-a91d-44e2-8051-9fc1aa8e23f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "falklands_data_path = falklands_data_dir / falklands_data_fname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d3f1b615-68c8-4d5f-8bba-fc517c4c9286",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PosixPath('/project/informatics_lab/data_science_cop/ML_challenges/2021_opmet_challenge/Rotors/2021_met_office_aviation_rotors.csv')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(falklands_data_path.is_file())\n",
    "falklands_data_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "20fbc099-cd72-4dd6-ae32-5dfa67402a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "falklands_df = pandas.read_csv(falklands_data_path, header=0).loc[1:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "be3746c7-6779-4874-976d-866af73d7155",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'Rotors 1 is true' in falklands_df.columns:\n",
    "    falklands_df = falklands_df.rename({'Rotors 1 is true': target_feature_name},axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f6badec5-0b8e-41ee-b45c-9087c4defdfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "falklands_df[target_feature_name]  = falklands_df[target_feature_name] .astype(bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "21956cda-c18f-4930-9ae4-8fcc1d999463",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Create date and times using the pandas datetime functionality\n",
    "falklands_df['DTG'] = pandas.to_datetime(falklands_df['DTG'])\n",
    "\n",
    "# There are some duplicate dates and times, so we drop those\n",
    "falklands_df = falklands_df.drop_duplicates(subset=['DTG'])\n",
    "\n",
    "# get rid of data points without a date and time\n",
    "falklands_df = falklands_df[~falklands_df['DTG'].isnull()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e66a2186-dbee-48b8-b94d-00f2f0caca06",
   "metadata": {},
   "outputs": [],
   "source": [
    "falklands_df['time'] = falklands_df['DTG']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f92f5625-45e4-4c44-bd72-2d8cb59ae5c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1       2015-01-01 00:00:00\n",
       "2       2015-01-01 03:00:00\n",
       "3       2015-01-01 06:00:00\n",
       "4       2015-01-01 09:00:00\n",
       "5       2015-01-01 12:00:00\n",
       "                ...        \n",
       "20101   2020-12-31 06:00:00\n",
       "20102   2020-12-31 09:00:00\n",
       "20103   2020-12-31 12:00:00\n",
       "20104   2020-12-31 15:00:00\n",
       "20105   2021-01-01 00:00:00\n",
       "Name: time, Length: 17486, dtype: datetime64[ns]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "falklands_df['time']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "56280900-e6f4-4712-9e8c-1b523f77996d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  subset observation so that wind speed, air temp, wind direction and dewpoints\n",
    "# are all greater than 0  (temperatures are in Kelvin, which must be greater than 0)\n",
    "falklands_df = falklands_df[(falklands_df['wind_speed_obs'] >= 0.0) &\n",
    "                            (falklands_df['air_temp_obs'] >= 0.0) &\n",
    "                            (falklands_df['wind_direction_obs'] >= 0.0) &\n",
    "                            (falklands_df['dewpoint_obs'] >= 0.0) \n",
    "                           ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7ab63e83-b076-43aa-819a-019d99fa60fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DTG</th>\n",
       "      <th>air_temp_obs</th>\n",
       "      <th>dewpoint_obs</th>\n",
       "      <th>wind_direction_obs</th>\n",
       "      <th>wind_speed_obs</th>\n",
       "      <th>wind_gust_obs</th>\n",
       "      <th>air_temp_1</th>\n",
       "      <th>air_temp_2</th>\n",
       "      <th>air_temp_3</th>\n",
       "      <th>air_temp_4</th>\n",
       "      <th>...</th>\n",
       "      <th>windspd_18</th>\n",
       "      <th>winddir_19</th>\n",
       "      <th>windspd_19</th>\n",
       "      <th>winddir_20</th>\n",
       "      <th>windspd_20</th>\n",
       "      <th>winddir_21</th>\n",
       "      <th>windspd_21</th>\n",
       "      <th>winddir_22</th>\n",
       "      <th>windspd_22</th>\n",
       "      <th>rotors_present</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015-01-01 00:00:00</td>\n",
       "      <td>283.9</td>\n",
       "      <td>280.7</td>\n",
       "      <td>110.0</td>\n",
       "      <td>4.1</td>\n",
       "      <td>-9999999.0</td>\n",
       "      <td>284.000</td>\n",
       "      <td>283.625</td>\n",
       "      <td>283.250</td>\n",
       "      <td>282.625</td>\n",
       "      <td>...</td>\n",
       "      <td>5.8</td>\n",
       "      <td>341.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>334.0</td>\n",
       "      <td>6.1</td>\n",
       "      <td>330.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>329.0</td>\n",
       "      <td>5.8</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015-01-01 03:00:00</td>\n",
       "      <td>280.7</td>\n",
       "      <td>279.7</td>\n",
       "      <td>90.0</td>\n",
       "      <td>7.7</td>\n",
       "      <td>-9999999.0</td>\n",
       "      <td>281.500</td>\n",
       "      <td>281.250</td>\n",
       "      <td>280.750</td>\n",
       "      <td>280.250</td>\n",
       "      <td>...</td>\n",
       "      <td>6.8</td>\n",
       "      <td>344.0</td>\n",
       "      <td>5.3</td>\n",
       "      <td>348.0</td>\n",
       "      <td>3.8</td>\n",
       "      <td>360.0</td>\n",
       "      <td>3.2</td>\n",
       "      <td>12.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015-01-01 06:00:00</td>\n",
       "      <td>279.8</td>\n",
       "      <td>278.1</td>\n",
       "      <td>100.0</td>\n",
       "      <td>7.7</td>\n",
       "      <td>-9999999.0</td>\n",
       "      <td>279.875</td>\n",
       "      <td>279.625</td>\n",
       "      <td>279.125</td>\n",
       "      <td>278.625</td>\n",
       "      <td>...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>345.0</td>\n",
       "      <td>5.5</td>\n",
       "      <td>358.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>4.2</td>\n",
       "      <td>38.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015-01-01 09:00:00</td>\n",
       "      <td>279.9</td>\n",
       "      <td>277.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>7.2</td>\n",
       "      <td>-9999999.0</td>\n",
       "      <td>279.625</td>\n",
       "      <td>279.250</td>\n",
       "      <td>278.875</td>\n",
       "      <td>278.250</td>\n",
       "      <td>...</td>\n",
       "      <td>3.1</td>\n",
       "      <td>338.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>354.0</td>\n",
       "      <td>3.9</td>\n",
       "      <td>9.0</td>\n",
       "      <td>4.4</td>\n",
       "      <td>22.0</td>\n",
       "      <td>4.6</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2015-01-01 12:00:00</td>\n",
       "      <td>279.9</td>\n",
       "      <td>277.4</td>\n",
       "      <td>120.0</td>\n",
       "      <td>8.7</td>\n",
       "      <td>-9999999.0</td>\n",
       "      <td>279.250</td>\n",
       "      <td>278.875</td>\n",
       "      <td>278.375</td>\n",
       "      <td>277.875</td>\n",
       "      <td>...</td>\n",
       "      <td>1.6</td>\n",
       "      <td>273.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>303.0</td>\n",
       "      <td>2.3</td>\n",
       "      <td>329.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>338.0</td>\n",
       "      <td>2.4</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20101</th>\n",
       "      <td>2020-12-31 06:00:00</td>\n",
       "      <td>276.7</td>\n",
       "      <td>275.5</td>\n",
       "      <td>270.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>-9999999.0</td>\n",
       "      <td>277.875</td>\n",
       "      <td>277.750</td>\n",
       "      <td>277.625</td>\n",
       "      <td>277.500</td>\n",
       "      <td>...</td>\n",
       "      <td>12.1</td>\n",
       "      <td>223.0</td>\n",
       "      <td>11.8</td>\n",
       "      <td>221.0</td>\n",
       "      <td>11.4</td>\n",
       "      <td>219.0</td>\n",
       "      <td>11.3</td>\n",
       "      <td>215.0</td>\n",
       "      <td>11.4</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20102</th>\n",
       "      <td>2020-12-31 09:00:00</td>\n",
       "      <td>277.9</td>\n",
       "      <td>276.9</td>\n",
       "      <td>270.0</td>\n",
       "      <td>3.1</td>\n",
       "      <td>-9999999.0</td>\n",
       "      <td>277.875</td>\n",
       "      <td>277.625</td>\n",
       "      <td>277.875</td>\n",
       "      <td>277.875</td>\n",
       "      <td>...</td>\n",
       "      <td>10.2</td>\n",
       "      <td>230.0</td>\n",
       "      <td>10.8</td>\n",
       "      <td>230.0</td>\n",
       "      <td>11.6</td>\n",
       "      <td>227.0</td>\n",
       "      <td>12.3</td>\n",
       "      <td>222.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20103</th>\n",
       "      <td>2020-12-31 12:00:00</td>\n",
       "      <td>283.5</td>\n",
       "      <td>277.1</td>\n",
       "      <td>220.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>-9999999.0</td>\n",
       "      <td>281.125</td>\n",
       "      <td>280.625</td>\n",
       "      <td>280.125</td>\n",
       "      <td>279.625</td>\n",
       "      <td>...</td>\n",
       "      <td>10.3</td>\n",
       "      <td>218.0</td>\n",
       "      <td>11.9</td>\n",
       "      <td>221.0</td>\n",
       "      <td>12.8</td>\n",
       "      <td>222.0</td>\n",
       "      <td>11.9</td>\n",
       "      <td>225.0</td>\n",
       "      <td>10.6</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20104</th>\n",
       "      <td>2020-12-31 15:00:00</td>\n",
       "      <td>286.1</td>\n",
       "      <td>276.9</td>\n",
       "      <td>250.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>-9999999.0</td>\n",
       "      <td>284.625</td>\n",
       "      <td>284.125</td>\n",
       "      <td>283.625</td>\n",
       "      <td>283.000</td>\n",
       "      <td>...</td>\n",
       "      <td>9.4</td>\n",
       "      <td>218.0</td>\n",
       "      <td>8.6</td>\n",
       "      <td>212.0</td>\n",
       "      <td>8.3</td>\n",
       "      <td>218.0</td>\n",
       "      <td>8.7</td>\n",
       "      <td>226.0</td>\n",
       "      <td>10.1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20105</th>\n",
       "      <td>2021-01-01 00:00:00</td>\n",
       "      <td>285.1</td>\n",
       "      <td>279.3</td>\n",
       "      <td>300.0</td>\n",
       "      <td>6.2</td>\n",
       "      <td>-9999999.0</td>\n",
       "      <td>284.250</td>\n",
       "      <td>284.000</td>\n",
       "      <td>283.750</td>\n",
       "      <td>283.250</td>\n",
       "      <td>...</td>\n",
       "      <td>8.6</td>\n",
       "      <td>241.0</td>\n",
       "      <td>10.2</td>\n",
       "      <td>236.0</td>\n",
       "      <td>10.5</td>\n",
       "      <td>232.0</td>\n",
       "      <td>10.5</td>\n",
       "      <td>227.0</td>\n",
       "      <td>11.3</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17486 rows × 95 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      DTG  air_temp_obs  dewpoint_obs  wind_direction_obs  \\\n",
       "1     2015-01-01 00:00:00         283.9         280.7               110.0   \n",
       "2     2015-01-01 03:00:00         280.7         279.7                90.0   \n",
       "3     2015-01-01 06:00:00         279.8         278.1               100.0   \n",
       "4     2015-01-01 09:00:00         279.9         277.0               120.0   \n",
       "5     2015-01-01 12:00:00         279.9         277.4               120.0   \n",
       "...                   ...           ...           ...                 ...   \n",
       "20101 2020-12-31 06:00:00         276.7         275.5               270.0   \n",
       "20102 2020-12-31 09:00:00         277.9         276.9               270.0   \n",
       "20103 2020-12-31 12:00:00         283.5         277.1               220.0   \n",
       "20104 2020-12-31 15:00:00         286.1         276.9               250.0   \n",
       "20105 2021-01-01 00:00:00         285.1         279.3               300.0   \n",
       "\n",
       "       wind_speed_obs  wind_gust_obs  air_temp_1  air_temp_2  air_temp_3  \\\n",
       "1                 4.1     -9999999.0     284.000     283.625     283.250   \n",
       "2                 7.7     -9999999.0     281.500     281.250     280.750   \n",
       "3                 7.7     -9999999.0     279.875     279.625     279.125   \n",
       "4                 7.2     -9999999.0     279.625     279.250     278.875   \n",
       "5                 8.7     -9999999.0     279.250     278.875     278.375   \n",
       "...               ...            ...         ...         ...         ...   \n",
       "20101             3.6     -9999999.0     277.875     277.750     277.625   \n",
       "20102             3.1     -9999999.0     277.875     277.625     277.875   \n",
       "20103             3.6     -9999999.0     281.125     280.625     280.125   \n",
       "20104             3.6     -9999999.0     284.625     284.125     283.625   \n",
       "20105             6.2     -9999999.0     284.250     284.000     283.750   \n",
       "\n",
       "       air_temp_4  ...  windspd_18  winddir_19  windspd_19  winddir_20  \\\n",
       "1         282.625  ...         5.8       341.0         6.0       334.0   \n",
       "2         280.250  ...         6.8       344.0         5.3       348.0   \n",
       "3         278.625  ...         6.0       345.0         5.5       358.0   \n",
       "4         278.250  ...         3.1       338.0         3.5       354.0   \n",
       "5         277.875  ...         1.6       273.0         2.0       303.0   \n",
       "...           ...  ...         ...         ...         ...         ...   \n",
       "20101     277.500  ...        12.1       223.0        11.8       221.0   \n",
       "20102     277.875  ...        10.2       230.0        10.8       230.0   \n",
       "20103     279.625  ...        10.3       218.0        11.9       221.0   \n",
       "20104     283.000  ...         9.4       218.0         8.6       212.0   \n",
       "20105     283.250  ...         8.6       241.0        10.2       236.0   \n",
       "\n",
       "       windspd_20  winddir_21  windspd_21  winddir_22  windspd_22  \\\n",
       "1             6.1       330.0         6.0       329.0         5.8   \n",
       "2             3.8       360.0         3.2        12.0         3.5   \n",
       "3             5.0        10.0         4.2        38.0         4.0   \n",
       "4             3.9         9.0         4.4        22.0         4.6   \n",
       "5             2.3       329.0         2.5       338.0         2.4   \n",
       "...           ...         ...         ...         ...         ...   \n",
       "20101        11.4       219.0        11.3       215.0        11.4   \n",
       "20102        11.6       227.0        12.3       222.0        12.0   \n",
       "20103        12.8       222.0        11.9       225.0        10.6   \n",
       "20104         8.3       218.0         8.7       226.0        10.1   \n",
       "20105        10.5       232.0        10.5       227.0        11.3   \n",
       "\n",
       "       rotors_present  \n",
       "1                True  \n",
       "2                True  \n",
       "3                True  \n",
       "4                True  \n",
       "5                True  \n",
       "...               ...  \n",
       "20101            True  \n",
       "20102            True  \n",
       "20103            True  \n",
       "20104            True  \n",
       "20105            True  \n",
       "\n",
       "[17486 rows x 95 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "falklands_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ddb4bccc-0ca8-4fcc-9447-75c436de6f63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DTG',\n",
       " 'air_temp_obs',\n",
       " 'dewpoint_obs',\n",
       " 'wind_direction_obs',\n",
       " 'wind_speed_obs',\n",
       " 'wind_gust_obs',\n",
       " 'air_temp_1',\n",
       " 'air_temp_2',\n",
       " 'air_temp_3',\n",
       " 'air_temp_4',\n",
       " 'air_temp_5',\n",
       " 'air_temp_6',\n",
       " 'air_temp_7',\n",
       " 'air_temp_8',\n",
       " 'air_temp_9',\n",
       " 'air_temp_10',\n",
       " 'air_temp_11',\n",
       " 'air_temp_12',\n",
       " 'air_temp_13',\n",
       " 'air_temp_14',\n",
       " 'air_temp_15',\n",
       " 'air_temp_16',\n",
       " 'air_temp_17',\n",
       " 'air_temp_18',\n",
       " 'air_temp_19',\n",
       " 'air_temp_20',\n",
       " 'air_temp_21',\n",
       " 'air_temp_22',\n",
       " 'sh_1',\n",
       " 'sh_2',\n",
       " 'sh_3',\n",
       " 'sh_4',\n",
       " 'sh_5',\n",
       " 'sh_6',\n",
       " 'sh_7',\n",
       " 'sh_8',\n",
       " 'sh_9',\n",
       " 'sh_10',\n",
       " 'sh_11',\n",
       " 'sh_12',\n",
       " 'sh_13',\n",
       " 'sh_14',\n",
       " 'sh_15',\n",
       " 'sh_16',\n",
       " 'sh_17',\n",
       " 'sh_18',\n",
       " 'sh_19',\n",
       " 'sh_20',\n",
       " 'sh_21',\n",
       " 'sh_22',\n",
       " 'winddir_1',\n",
       " 'windspd_1',\n",
       " 'winddir_2',\n",
       " 'windspd_2',\n",
       " 'winddir_3',\n",
       " 'windspd_3',\n",
       " 'winddir_4',\n",
       " 'windspd_4',\n",
       " 'winddir_5',\n",
       " 'windspd_5',\n",
       " 'winddir_6',\n",
       " 'windspd_6',\n",
       " 'winddir_7',\n",
       " 'windspd_7',\n",
       " 'winddir_8',\n",
       " 'windspd_8',\n",
       " 'winddir_9',\n",
       " 'windspd_9',\n",
       " 'winddir_10',\n",
       " 'windspd_10',\n",
       " 'winddir_11',\n",
       " 'windspd_11',\n",
       " 'winddir_12',\n",
       " 'windspd_12',\n",
       " 'winddir_13',\n",
       " 'windspd_13',\n",
       " 'winddir_14',\n",
       " 'windspd_14',\n",
       " 'winddir_15',\n",
       " 'windspd_15',\n",
       " 'winddir_16',\n",
       " 'windspd_16',\n",
       " 'winddir_17',\n",
       " 'windspd_17',\n",
       " 'winddir_18',\n",
       " 'windspd_18',\n",
       " 'winddir_19',\n",
       " 'windspd_19',\n",
       " 'winddir_20',\n",
       " 'windspd_20',\n",
       " 'winddir_21',\n",
       " 'windspd_21',\n",
       " 'winddir_22',\n",
       " 'windspd_22',\n",
       " 'rotors_present',\n",
       " 'u_wind_1',\n",
       " 'v_wind_1',\n",
       " 'u_wind_2',\n",
       " 'v_wind_2',\n",
       " 'u_wind_3',\n",
       " 'v_wind_3',\n",
       " 'u_wind_4',\n",
       " 'v_wind_4',\n",
       " 'u_wind_5',\n",
       " 'v_wind_5',\n",
       " 'u_wind_6',\n",
       " 'v_wind_6',\n",
       " 'u_wind_7',\n",
       " 'v_wind_7',\n",
       " 'u_wind_8',\n",
       " 'v_wind_8',\n",
       " 'u_wind_9',\n",
       " 'v_wind_9',\n",
       " 'u_wind_10',\n",
       " 'v_wind_10',\n",
       " 'u_wind_11',\n",
       " 'v_wind_11',\n",
       " 'u_wind_12',\n",
       " 'v_wind_12',\n",
       " 'u_wind_13',\n",
       " 'v_wind_13',\n",
       " 'u_wind_14',\n",
       " 'v_wind_14',\n",
       " 'u_wind_15',\n",
       " 'v_wind_15',\n",
       " 'u_wind_16',\n",
       " 'v_wind_16',\n",
       " 'u_wind_17',\n",
       " 'v_wind_17',\n",
       " 'u_wind_18',\n",
       " 'v_wind_18',\n",
       " 'u_wind_19',\n",
       " 'v_wind_19',\n",
       " 'u_wind_20',\n",
       " 'v_wind_20',\n",
       " 'u_wind_21',\n",
       " 'v_wind_21',\n",
       " 'u_wind_22',\n",
       " 'v_wind_22',\n",
       " 'u_wind_obs',\n",
       " 'v_wind_obs',\n",
       " 'test_set']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(falklands_df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b8a49f2-eb8e-4184-aac4-97f426a718e9",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e38f0d-e90d-4b3c-ac05-71ccb0b21969",
   "metadata": {},
   "source": [
    "Having loaded the data, we then do some preprocessing. This includes:\n",
    "* Specify feature names\n",
    "* convert wind speed / direction back to u/v wind. This is because these parameters will vary more smoothly for northerly winds, which is the wind we are interested in.\n",
    "* prepare the target variable, including filling in missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ef7b166a-e066-4da5-b40b-5139eec457dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_names = [\n",
    "    'air_temp_obs',\n",
    "    'dewpoint_obs',\n",
    "    'wind_speed_obs',\n",
    "    'wind_direction_obs',\n",
    "]\n",
    "\n",
    "obs_feature_names = [\n",
    "    'air_temp_obs',\n",
    "    'dewpoint_obs',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "76d73fbf-1d43-4a04-864b-ff32161619f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_v_wind(wind_dir_name, wind_speed_name, row1):\n",
    "    return math.cos(math.radians(row1[wind_dir_name])) * row1[wind_speed_name]\n",
    "\n",
    "def get_u_wind(wind_dir_name, wind_speed_name, row1):\n",
    "    return math.sin(math.radians(row1[wind_dir_name])) * row1[wind_speed_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "73a632c6-89ef-46a1-9730-8a1f6c7e6e0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 23.2 s, sys: 2.65 s, total: 25.9 s\n",
      "Wall time: 26 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "u_feature_template = 'u_wind_{level_ix}'\n",
    "v_feature_template = 'v_wind_{level_ix}'\n",
    "u_wind_feature_names = []\n",
    "v_wind_features_names = []\n",
    "for wsn1, wdn1 in zip(wind_speed_feature_names, wind_direction_feature_names):\n",
    "    level_ix = int( wsn1.split('_')[1])\n",
    "    u_feature = u_feature_template.format(level_ix=level_ix)\n",
    "    u_wind_feature_names += [u_feature]\n",
    "    falklands_df[u_feature] = falklands_df.apply(functools.partial(get_u_wind, wdn1, wsn1), axis='columns')\n",
    "    v_feature = v_feature_template.format(level_ix=level_ix)\n",
    "    v_wind_features_names += [v_feature]\n",
    "    falklands_df[v_feature] = falklands_df.apply(functools.partial(get_v_wind, wdn1, wsn1), axis='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d9380ded-6669-450d-ac50-0ac74247ee2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "wdn1 = 'wind_direction_obs'\n",
    "wsn1 = 'wind_speed_obs'\n",
    "u_feature = u_feature_template.format(level_ix='obs')\n",
    "obs_feature_names += [u_feature]\n",
    "falklands_df[u_feature] = falklands_df.apply(functools.partial(get_u_wind, wdn1, wsn1), axis='columns')\n",
    "v_feature = v_feature_template.format(level_ix='obs')\n",
    "obs_feature_names += [v_feature]\n",
    "falklands_df[v_feature] = falklands_df.apply(functools.partial(get_v_wind, wdn1, wsn1), axis='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "848ccb16-717b-492b-9efb-a2da6da62a9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>air_temp_obs</th>\n",
       "      <th>dewpoint_obs</th>\n",
       "      <th>u_wind_obs</th>\n",
       "      <th>v_wind_obs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>283.9</td>\n",
       "      <td>280.7</td>\n",
       "      <td>3.852740</td>\n",
       "      <td>-1.402283e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>280.7</td>\n",
       "      <td>279.7</td>\n",
       "      <td>7.700000</td>\n",
       "      <td>4.714890e-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>279.8</td>\n",
       "      <td>278.1</td>\n",
       "      <td>7.583020</td>\n",
       "      <td>-1.337091e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>279.9</td>\n",
       "      <td>277.0</td>\n",
       "      <td>6.235383</td>\n",
       "      <td>-3.600000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>279.9</td>\n",
       "      <td>277.4</td>\n",
       "      <td>7.534421</td>\n",
       "      <td>-4.350000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20101</th>\n",
       "      <td>276.7</td>\n",
       "      <td>275.5</td>\n",
       "      <td>-3.600000</td>\n",
       "      <td>-6.613093e-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20102</th>\n",
       "      <td>277.9</td>\n",
       "      <td>276.9</td>\n",
       "      <td>-3.100000</td>\n",
       "      <td>-5.694608e-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20103</th>\n",
       "      <td>283.5</td>\n",
       "      <td>277.1</td>\n",
       "      <td>-2.314035</td>\n",
       "      <td>-2.757760e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20104</th>\n",
       "      <td>286.1</td>\n",
       "      <td>276.9</td>\n",
       "      <td>-3.382893</td>\n",
       "      <td>-1.231273e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20105</th>\n",
       "      <td>285.1</td>\n",
       "      <td>279.3</td>\n",
       "      <td>-5.369358</td>\n",
       "      <td>3.100000e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17486 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       air_temp_obs  dewpoint_obs  u_wind_obs    v_wind_obs\n",
       "1             283.9         280.7    3.852740 -1.402283e+00\n",
       "2             280.7         279.7    7.700000  4.714890e-16\n",
       "3             279.8         278.1    7.583020 -1.337091e+00\n",
       "4             279.9         277.0    6.235383 -3.600000e+00\n",
       "5             279.9         277.4    7.534421 -4.350000e+00\n",
       "...             ...           ...         ...           ...\n",
       "20101         276.7         275.5   -3.600000 -6.613093e-16\n",
       "20102         277.9         276.9   -3.100000 -5.694608e-16\n",
       "20103         283.5         277.1   -2.314035 -2.757760e+00\n",
       "20104         286.1         276.9   -3.382893 -1.231273e+00\n",
       "20105         285.1         279.3   -5.369358  3.100000e+00\n",
       "\n",
       "[17486 rows x 4 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "falklands_df[obs_feature_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9d34e3d4-5e76-4ada-9c54-623f72444b6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1        True\n",
       "2        True\n",
       "3        True\n",
       "4        True\n",
       "5        True\n",
       "         ... \n",
       "20101    True\n",
       "20102    True\n",
       "20103    True\n",
       "20104    True\n",
       "20105    True\n",
       "Name: rotors_present, Length: 17486, dtype: bool"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "falklands_df[target_feature_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7f5da24f-8047-4cca-b696-37b6afe6cd57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True    17486\n",
       "Name: rotors_present, dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "falklands_df[target_feature_name].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ee2296a3-a8bd-4ca8-b9d7-915585324f7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['DTG', 'air_temp_obs', 'dewpoint_obs', 'wind_direction_obs',\n",
       "       'wind_speed_obs', 'wind_gust_obs', 'air_temp_1', 'air_temp_2',\n",
       "       'air_temp_3', 'air_temp_4',\n",
       "       ...\n",
       "       'u_wind_19', 'v_wind_19', 'u_wind_20', 'v_wind_20', 'u_wind_21',\n",
       "       'v_wind_21', 'u_wind_22', 'v_wind_22', 'u_wind_obs', 'v_wind_obs'],\n",
       "      dtype='object', length=141)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "falklands_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4424f4d2-5ee6-462b-bdec-60da81b5c753",
   "metadata": {},
   "source": [
    "## Train/test Split\n",
    "\n",
    "Splitting data into train/validation/test sets\n",
    "\n",
    "To consider \n",
    "* consistency of distributions\n",
    "* class imbalance\n",
    "* correlation between samples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60098da2-8d5c-49a6-85d7-7d8bc7623f54",
   "metadata": {},
   "source": [
    "An initial option might be to split randomly, using the built-in scikit learn functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6dbd1a79-51bd-40ce-95c5-633004f01097",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_fraction = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "488b0290-d87c-46fa-884b-215d0bebbba9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((13114, 141), (4372, 141))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_random_df, test_random_df = sklearn.model_selection.train_test_split(falklands_df)\n",
    "train_random_df.shape, test_random_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e040aa78-80b4-455b-a7ee-41f82c98d5e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True    13114\n",
       "Name: rotors_present, dtype: int64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_random_df[target_feature_name].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d16cbe98-430c-42d0-ab04-d8d0f0acf3df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True    4372\n",
       "Name: rotors_present, dtype: int64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_random_df[target_feature_name].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7548195-49b8-4e20-802b-9423a526e94e",
   "metadata": {},
   "source": [
    "We know that our 2 classes (rotor detected/ no rotor detected) are imbalanced, so we might want to select from each class, to ensure our train/test splits have distributions which reflect the larger distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "dad10ddd-372e-4785-bb96-821cf4e5769b",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_no_rotors = sum(falklands_df[target_feature_name] == False)\n",
    "num_with_rotors = sum(falklands_df[target_feature_name] == True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d0da0960-74c2-46f3-9441-c49e3fd5d736",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 17486)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_no_rotors,num_with_rotors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5e9b35d9-7176-407d-8d4c-deed6af86f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_no_rotors = falklands_df[falklands_df[target_feature_name] == False]\n",
    "data_with_rotors = falklands_df[falklands_df[target_feature_name] == True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d5a6cac1-8c2c-40bd-95af-1191abddf514",
   "metadata": {},
   "outputs": [],
   "source": [
    "falklands_df['test_set'] = False\n",
    "falklands_df.loc[list(data_no_rotors.sample(frac=test_fraction).index) + list(data_with_rotors.sample(frac=test_fraction).index),'test_set'] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "aa79b05a-acd9-4a76-81a9-295e6bd54fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_class_df = falklands_df[falklands_df['test_set'] == False]\n",
    "test_class_df = falklands_df[falklands_df['test_set'] == True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "503f53d9-e553-4eb4-982b-9389f5bc7537",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True    13989\n",
       "Name: rotors_present, dtype: int64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_class_df[target_feature_name].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d90b87f5-6ed2-4ad2-aaf0-6667749662d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True    3497\n",
       "Name: rotors_present, dtype: int64"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_class_df[target_feature_name].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f7460ea-f3f0-47f4-a4e5-1318138d874f",
   "metadata": {},
   "source": [
    "We also know though that data points from adjacent points in time are likely to be correlated. As a result dfata in our test set will be correlated with that in our train set if we split randomly. Instead for this problem, we should split by time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7196ee19-b04b-4e87-ab27-f1c1cc696e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = falklands_df[falklands_df['time'] < datetime.datetime(2020,1,1,0,0)]\n",
    "test_df = falklands_df[falklands_df['time'] > datetime.datetime(2020,1,1,0,0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "51d94542-d781-4aa2-bb04-af8017bd20d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True    14572\n",
       "Name: rotors_present, dtype: int64"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[target_feature_name].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "251296a7-2fc5-4472-8786-0128229aa195",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True    2913\n",
       "Name: rotors_present, dtype: int64"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df[target_feature_name].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95bee85d-0e9a-4bc0-bb8d-a371ee72e024",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "56aeabfc-e039-4342-b42c-67b14e47da20",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_feature_names = temp_feature_names + humidity_feature_names + u_wind_feature_names + v_wind_features_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c1d63d4b-e984-4d0e-8d17-ced7884418cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "preproc_dict = {}\n",
    "for if1 in input_feature_names:\n",
    "    scaler1 = sklearn.preprocessing.StandardScaler()\n",
    "    scaler1.fit(train_df[[if1]])\n",
    "    preproc_dict[if1] = scaler1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "897ff819-9a9b-41cc-90de-bd87607d2b22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_encoder = sklearn.preprocessing.LabelEncoder()\n",
    "target_encoder.fit(train_df[[target_feature_name]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e779f9d6-bdb9-4376-a3f6-c71b426620a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preproc_input(data_subset, pp_dict):\n",
    "    return numpy.concatenate([scaler1.transform(data_subset[[if1]]) for if1,scaler1 in pp_dict.items()],axis=1)\n",
    "\n",
    "def preproc_target(data_subset, enc1):\n",
    "     return enc1.transform(data_subset[[target_feature_name]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ef982738-d95a-4c66-a480-cfdabf600a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = preproc_input(train_df, preproc_dict)\n",
    "y_train = preproc_target(train_df, target_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "57734f05-af8d-438c-b9c5-7cf24efcd533",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((14572,), (14572, 88))"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape, X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "98e73f79-8c49-4715-b806-c209b1eea741",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(y_train == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e94c53e5-df08-4f33-9eb3-1e0fa8c67238",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = preproc_input(test_df, preproc_dict)\n",
    "y_test = preproc_target(test_df, target_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f051cff4-8498-498c-a081-2c2bef755596",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2913,), (2913, 88))"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "d4aa844a-d198-476e-afb6-7a6328dc7020",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_tuples = [\n",
    "    (X_train, y_train),\n",
    "    (X_test, y_test),    \n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a312cb28-8847-4934-85ee-36b81f2ad349",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Algorithm Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "82ed1fb1-901b-4229-a63f-6fb6e8d81354",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers_params = {\n",
    "    'decision_tree': {'class': sklearn.tree.DecisionTreeClassifier, 'opts': {'max_depth':10, 'class_weight':'balanced'}},\n",
    "    'random_forest': {'class': sklearn.ensemble.RandomForestClassifier, 'opts': {'max_depth':10, 'class_weight':'balanced'}},\n",
    "     'ann_5_500': {'class': sklearn.neural_network.MLPClassifier, 'opts': {'hidden_layer_sizes':(500,500,500,500,500)}},   \n",
    "     'ann_3_700': {'class': sklearn.neural_network.MLPClassifier, 'opts': {'hidden_layer_sizes':(700,700,700)}},   \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c9a03fd3-5a30-4307-8ccb-fb104f1a947b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 129 µs, sys: 15 µs, total: 144 µs\n",
      "Wall time: 152 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "classifiers_dict = {}             \n",
    "for clf_name, clf_params in classifiers_params.items():\n",
    "    clf1 = clf_params['class'](**clf_params['opts'])\n",
    "    classifiers_dict[clf_name] = clf1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c231d98-4881-439b-bd0a-ab27a8b0fc5b",
   "metadata": {},
   "source": [
    "## Algorithm Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "3bcc70f8-8551-42ef-ad21-a8027f6df7fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decision_tree\n",
      "random_forest\n",
      "ann_5_500\n",
      "ann_3_700\n",
      "CPU times: user 5min 36s, sys: 2min 22s, total: 7min 59s\n",
      "Wall time: 2min 1s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for clf_name, clf1 in classifiers_dict.items():\n",
    "    print(clf_name)\n",
    "    clf1.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610aff10-dee4-405c-b805-218fbe6cc4b9",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "57661dd6-26da-49e5-8556-12b46baf74b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted classes train set: [14572]\n",
      "actual classes train set: [14572]\n",
      "predicted classes train set: [14572]\n",
      "actual classes train set: [14572]\n",
      "predicted classes train set: [14572]\n",
      "actual classes train set: [14572]\n",
      "predicted classes train set: [14572]\n",
      "actual classes train set: [14572]\n"
     ]
    }
   ],
   "source": [
    "y_pred_train = {}\n",
    "\n",
    "for clf_name, clf1 in classifiers_dict.items():\n",
    "    y_pred = clf1.predict(X_train)\n",
    "    y_pred_train[clf_name] = y_pred\n",
    "    train_df[f'pred_{clf_name}'] = y_pred\n",
    "    rotors_freq = list(train_df[f'pred_{clf_name}'].value_counts())\n",
    "    print(f'predicted classes train set: {rotors_freq}')\n",
    "    actual_freq = list(train_df[target_feature_name].value_counts())\n",
    "    print(f'actual classes train set: {actual_freq}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "0035f496-254b-429c-856d-1f4689794dc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted classes test set: [2913]\n",
      "actual classes test set: [2913]\n",
      "predicted classes test set: [2913]\n",
      "actual classes test set: [2913]\n",
      "predicted classes test set: [2913]\n",
      "actual classes test set: [2913]\n",
      "predicted classes test set: [2913]\n",
      "actual classes test set: [2913]\n"
     ]
    }
   ],
   "source": [
    "y_pred_test = {}\n",
    "\n",
    "for clf_name, clf1 in classifiers_dict.items():\n",
    "    y_pred = clf1.predict(X_test)\n",
    "    y_pred_test[clf_name] = y_pred\n",
    "    test_df[f'pred_{clf_name}'] = y_pred\n",
    "    rotors_freq = list(test_df[f'pred_{clf_name}'].value_counts())\n",
    "    print(f'predicted classes test set: {rotors_freq}')\n",
    "    actual_freq = list(test_df[target_feature_name].value_counts())\n",
    "    print(f'actual classes test set: {actual_freq}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de953fb7-7d79-4422-a938-61a3a6de80cd",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "To determine whether the algorithm is performing well, we need to measure the performance in some way. For supervised learning, metrics are divided by the two key types of problem: regression and classification. Whichever sort of algorithm you're using, getting good results depends on measuring the right thing that best reflects the real world impact, business need or physical reality of the system being studied. CHoosing the wrong metric might result in a trained algorithm that seemingly performs well in development, but isn't actually that useful in the real world."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4e1ee2-3c42-41fc-a9fb-fd32eba6b1aa",
   "metadata": {},
   "source": [
    "We're dealing with a classification problem here, so we will use classfication relevant metrics.\n",
    "The obvious metric to use is accuracy i.e. how many predictions did we get right? As we will see, this  usually does represent the way want the algorithm to perform. Obviously we want all the predictions to be correct, but generally, some will be wrong. Depending on the impact of incorrect predictions, we may wish penalise certain mistakes more or less. Different sorts of metrics allow us to do that.\n",
    "    \n",
    "A common starting point is precision and recall. This is frequently confusing, and the [wikipedia page](https://en.wikipedia.org/wiki/Precision_and_recall) has numerous useful explanations and diagrams. Precision is the portion of data points predicted as being in a class that are actually in that class. *Recall* in the proportion of data points tat are in a specific class in reality that are predicted as being in that class.\n",
    "![Precision and recall](images/Precisionrecall.svg)\n",
    "\n",
    "Graphic taken from Wikipedia\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "f92c2875-1425-479c-a03a-aefc278bd383",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision=[1.],\n",
      " recall=[1.],\n",
      " f1-score=[1.],\n",
      " support=[14572] \n"
     ]
    }
   ],
   "source": [
    "prec, recall, f1, support = sklearn.metrics.precision_recall_fscore_support(\n",
    "    y_train,\n",
    "    classifiers_dict['decision_tree'].predict(X_train), \n",
    ")\n",
    "\n",
    "print(f'precision={prec},\\n recall={recall},\\n f1-score={f1},\\n support={support} ')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a1bc24-08ce-47ac-8a38-3cd9355cede9",
   "metadata": {},
   "source": [
    "Here we see that we have poor precision and good recall from decision tree. This means that we are predicting a high percentage of all actual rotor events, but we are also predicting as rotor events quite a lot of data points that are not actually rotor events.\n",
    "\n",
    "F1-score combines precision and recall to create a balanced metric. \n",
    "\n",
    "$F_{1}= 2 * \\frac{precision * recall}{precision + recall}$\n",
    "\n",
    "The One in the name reflects equal weighting of precision and recall in the metric. We can start to come up with more sophisticated metrics by giving more weight to one or the other, in the $F_{\\beta}$ metric\n",
    "\n",
    "$F_{\\beta} = (1 + \\beta)^2 * \\frac{precision * recall}{\\beta^2 *precision + recall}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43743655-58eb-4049-99ae-e869b69a1187",
   "metadata": {},
   "source": [
    "Another way to look at the results is using a confusioin matrix, also referenced in the wikipedia page above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "7b931c3c-e58a-42aa-8aa5-efef0b302743",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[14572]])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sklearn.metrics.confusion_matrix(\n",
    "    y_train,\n",
    "    classifiers_dict['decision_tree'].predict(X_train),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "e7056bab-e71b-4f06-b7aa-ea4e9a3527c7",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 4, got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [72]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m true_negatives, false_positives, false_negatives, true_positives \u001b[38;5;241m=\u001b[39m sklearn\u001b[38;5;241m.\u001b[39mmetrics\u001b[38;5;241m.\u001b[39mconfusion_matrix(\n\u001b[1;32m      2\u001b[0m     y_train,\n\u001b[1;32m      3\u001b[0m     classifiers_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdecision_tree\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mpredict(X_train),\n\u001b[1;32m      4\u001b[0m )\u001b[38;5;241m.\u001b[39mravel()\n\u001b[1;32m      5\u001b[0m true_negatives, false_positives, false_negatives, true_positives\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 4, got 1)"
     ]
    }
   ],
   "source": [
    "true_negatives, false_positives, false_negatives, true_positives = sklearn.metrics.confusion_matrix(\n",
    "    y_train,\n",
    "    classifiers_dict['decision_tree'].predict(X_train),\n",
    ").ravel()\n",
    "true_negatives, false_positives, false_negatives, true_positives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9512c2d4-9dfe-4e4b-ae00-eef9940ec89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm1 = sklearn.metrics.confusion_matrix(\n",
    "    y_train,\n",
    "    classifiers_dict['decision_tree'].predict(X_train),\n",
    ")\n",
    "cm1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4abf4ecb-a833-4581-9d90-08e8e834b2f1",
   "metadata": {},
   "source": [
    "The confusion matrix shows us where the algorithm correctly predicts Yes or No for a class, and where it gets it wrong.\n",
    "* top left = true negative - Model predicts no rotors where none were observed\n",
    "* top right = false postive (false alarm) - Model predicts rotors where no rotors were observed\n",
    "* bottom left = false negative (miss) - Model predicts no rotors whererotors were actually observed\n",
    "* bottom right = true positive (hit) - Model predicts rotors where rotors were observed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe6bc4c-fa5e-46d1-8220-8d7a83f6ecde",
   "metadata": {},
   "source": [
    "We can use this as the basis for other useful metrics. Hit rate is the proportion of data points where rotors were observed where rotors were actually predicted by the algorithm. The higher the hit rate, the more trust there will be in any positive predictions of rotors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f571f67a-4759-47db-9804-f070dd51fd63",
   "metadata": {},
   "outputs": [],
   "source": [
    "hit_rate = cm1[1,1] / (cm1[1,1]+cm1[1,0])\n",
    "hit_rate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c6e39e-258a-4df5-a129-a0cf461634a1",
   "metadata": {},
   "source": [
    "False alarm rate is proportion of data points where no rotors were observed where the model incorrectly predicted rotors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8caf79c-2441-4af3-b632-b84086544501",
   "metadata": {},
   "outputs": [],
   "source": [
    "false_alarm_rate = cm1[0,1] / (cm1[0,1]+cm1[0,0])\n",
    "false_alarm_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af003541-7eb1-41fe-942f-505a00b5d062",
   "metadata": {},
   "source": [
    "The question then, is what do we really care about in this problem? What sort of errors are the most impactful? In our case, it is very important not to miss any rotor events, so false neagatives are very bad and should be penalised. False positive errode trust in the forecast, but it is less costly to predict a few events that don't happen than to miss any events at all. So we might consider the False Alarm and the Miss rate to be very important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2860845f-af06-49ba-be2e-2835a7d00638",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics1 = []\n",
    "for X1, y1 in train_test_tuples:\n",
    "    md1 = {'classifier': [],\n",
    "           'precision_noRotor': [], 'precision_rotor': [],\n",
    "           'recall_noRotor': [], 'recall_rotor': [], \n",
    "           'f1_noRotor': [], 'f1_rotor': [], \n",
    "           'hit_rate': [], 'false_alarm_rate': []\n",
    "          }\n",
    "    for clf_name, clf1 in classifiers_dict.items():\n",
    "        md1['classifier'] += [clf_name]\n",
    "        prec, recall, f1, support = sklearn.metrics.precision_recall_fscore_support(clf1.predict(X1), y1)\n",
    "        md1['precision_noRotor'] += [prec[0]]\n",
    "        md1['precision_rotor'] += [prec[1]]\n",
    "        md1['recall_noRotor'] += [recall[0]]\n",
    "        md1['recall_rotor'] += [recall[1]]\n",
    "        md1['f1_noRotor'] += [f1[0]]\n",
    "        md1['f1_rotor'] += [f1[1]]\n",
    "        cm1 = sklearn.metrics.confusion_matrix(clf1.predict(X1), y1)\n",
    "        hit_rate = cm1[1,1] / (cm1[1,1]+cm1[1,0])\n",
    "        md1['hit_rate'] += [hit_rate]\n",
    "        false_alarm_rate = cm1[0,1] / (cm1[0,1]+cm1[0,0])\n",
    "        md1['false_alarm_rate'] += [false_alarm_rate]\n",
    "    metrics1 += [md1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf350938-f8b0-46f1-ab90-0c052d6a0214",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_metrics_df = pandas.DataFrame(metrics1[0])\n",
    "test_metrics_df = pandas.DataFrame(metrics1[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6bfd084-d63b-4393-94eb-18f54a9c854a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1 = matplotlib.pyplot.figure(figsize=(16,20))\n",
    "ax1 = fig1.add_subplot(3,2,1,title='precision (train) for rotors present class')\n",
    "train_metrics_df.plot.bar(x='classifier', y='precision_rotor', ax=ax1, ylim=[0,1])\n",
    "ax1 = fig1.add_subplot(3,2,2,title='recall (train) for rotors present class')\n",
    "train_metrics_df.plot.bar(x='classifier', y='recall_rotor', ax=ax1, ylim=[0,1])\n",
    "\n",
    "ax1 = fig1.add_subplot(3,2,3,title='precision (test) for rotors present class')\n",
    "test_metrics_df.plot.bar(x='classifier', y='precision_rotor', ax=ax1, ylim=[0,1])\n",
    "ax1 = fig1.add_subplot(3,2,4,title='recall (test) for rotors present class')\n",
    "test_metrics_df.plot.bar(x='classifier', y='recall_rotor', ax=ax1, ylim=[0,1])\n",
    "\n",
    "ax1 = fig1.add_subplot(3,2,5,title='false alarm rate (train) for rotors present class')\n",
    "train_metrics_df.plot.bar(x='classifier', y='false_alarm_rate', ax=ax1, ylim=[0,1])\n",
    "ax1 = fig1.add_subplot(3,2,6,title='false alarm rate (test) for rotors present class')\n",
    "test_metrics_df.plot.bar(x='classifier', y='false_alarm_rate', ax=ax1, ylim=[0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87325cef-e3cf-4e6b-b9be-9e295adeec8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_sedi(conf_mat):\n",
    "    hr1 = conf_mat[1,1] / (conf_mat[1,0] + conf_mat[1,1])\n",
    "    fa1 = conf_mat[0,1] / (conf_mat[0,0] + conf_mat[0,1])\n",
    "    sedi_score1 = (\n",
    "        (numpy.log(fa1) - numpy.log(hr1) - numpy.log(1.0-fa1) + numpy.log(1.0-hr1) )\n",
    "        / (numpy.log(fa1) + numpy.log(hr1) + numpy.log(1.0 - fa1) + numpy.log(1.0-hr1) )  )\n",
    "    return sedi_score1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d396441-4dea-4762-96b6-278931aa13d8",
   "metadata": {},
   "source": [
    "How though do we actually go about getting better results? One way for this is to understand that some algorithms output a pseudo-probability, and the classification is based on thesholding that value. By altering the threshold we get different predictions and thus different metric results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed485e2-9000-4f2c-a249-145daca24031",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred_raw = classifiers_dict['random_forest'].predict_proba(X_train)[:,1]\n",
    "y_test_pred_raw = classifiers_dict['random_forest'].predict_proba(X_test)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a6ed6b0-24c8-4821-aed9-c72d563a4d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds_list = list(numpy.arange(1e-3,0.995,5e-3))\n",
    "hit_rates = []\n",
    "hit_rates_test = []\n",
    "false_alarm_rates = []\n",
    "false_alarm_rates_test = []\n",
    "sedi_list = []\n",
    "sedi_list_test = []\n",
    "for threshold in thresholds_list:\n",
    "    y_pred_train = list(map(float, y_train_pred_raw > threshold)) \n",
    "    cm1 = sklearn.metrics.confusion_matrix(y_train, y_pred_train)\n",
    "    hit_rates += [cm1[1,1] / (cm1[1,0] + cm1[1,1])]\n",
    "    false_alarm_rates += [cm1[0,1] / (cm1[0,0] + cm1[0,1])]\n",
    "    sedi_list += [calc_sedi(cm1)]\n",
    "\n",
    "    y_pred_test = list(map(float, y_test_pred_raw > threshold)) \n",
    "    cm1 = sklearn.metrics.confusion_matrix(y_test, y_pred_test)\n",
    "    hit_rates_test += [cm1[1,1] / (cm1[1,0] + cm1[1,1])]\n",
    "    false_alarm_rates_test += [cm1[0,1] / (cm1[0,0] + cm1[0,1])]\n",
    "    sedi_list_test += [calc_sedi(cm1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a8891c6-6d8d-414d-9298-bb2fe79578d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = matplotlib.pyplot.figure(figsize=(16,10))\n",
    "ax1 = f1.add_subplot(1,2,1,title='hit rates vs false alarms (training data)')\n",
    "ax1.plot(thresholds_list, hit_rates,'r')\n",
    "ax1.plot(thresholds_list, false_alarm_rates,'b')\n",
    "ax1.plot(thresholds_list, sedi_list,'k')\n",
    "\n",
    "ax1 = f1.add_subplot(1,2,2,title='hit rates vs false alarms (test data)')\n",
    "ax1.plot(thresholds_list, hit_rates_test,'r')\n",
    "ax1.plot(thresholds_list, false_alarm_rates_test,'b')\n",
    "ax1.plot(thresholds_list, sedi_list_test,'k')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b34b80b3-24b0-47bf-9ae0-439dcc6b63dd",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Model Storage\n",
    "Once we have trained a model, we usually want to save the result for subsequent use in scientific work. There a re a variety of ways of doing this. The most simple way in scikit learn is to pickle the data\n",
    "\n",
    "discuss what elements need to be stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb8e95f8-d37c-42ab-8c4c-62495ff88094",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import tempfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1876b99-f31f-460f-8bd3-3799294cc6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_str = pickle.dumps(classifiers_dict['random_forest'])\n",
    "rf_str[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d31056-c3a5-43eb-ba6f-52fe22e4341d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tempfile.TemporaryDirectory() as td1:\n",
    "    with open(pathlib.Path(td1) / 'my_trained_classifier.pkl','wb') as clf_file_out1:\n",
    "        clf_file_out1.write(rf_str)\n",
    "    with open(pathlib.Path(td1) / 'my_trained_classifier.pkl','rb') as clf_file_in1:\n",
    "        train_clf_str1 = clf_file_in1.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e79847-141a-42f5-808c-af2d97857a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_rf_clf = pickle.loads(train_clf_str1)\n",
    "loaded_rf_clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e385e43b-7f5d-4b6f-8dcd-fd7801be6cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_rf_clf.predict(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c79faf-c9b6-4218-922d-3ceee5277ce1",
   "metadata": {},
   "source": [
    "There are more sophisticated ways to do this.\n",
    "* [Tensorflow](https://www.tensorflow.org/guide/keras/save_and_serialize) has a more roust custom format for saving and loading models.\n",
    "* [ONNX](https://onnx.ai/) - A library to save models and perform inference\n",
    "* [ML Flow Models](https://www.mlflow.org/docs/latest/models.html) - Another library that can save models and do inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ac6f5c-8504-47b0-a8c3-f3e9c0eab525",
   "metadata": {},
   "source": [
    "## Problem 2 - Clustering weather types from ERA5\n",
    "\n",
    "So far we have looked an example of *supervised learning*, where we want to predict some target values from input values, which is what we call *labelled data*. Labelling is an expensive process, and where possible it is very useful to be able to find strcuture and patterns in unlabelled data. This can be particularly helpful to explore the data and find structures, as well as providing an approach to perform useful operations without the expense of labelling data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "181333cc-bbfe-4976-948b-9429882b7333",
   "metadata": {},
   "source": [
    "### Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8925fa9c-e48d-403d-9f7f-6c0e2be6dc90",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    era5_root = os.environ['ML_TUTORIAL_DIR']\n",
    "except KeyError:\n",
    "    era5_root = '/project/informatics_lab/data_science_cop/era5/'\n",
    "era5_data_dir = pathlib.Path(era5_root) \n",
    "print(era5_data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de34937e-fc41-4e7f-b4b0-91b25d62a326",
   "metadata": {},
   "outputs": [],
   "source": [
    "era5_mslp_path = era5_data_dir / 'era5_mslp_UK_2017_2020.nc'\n",
    "print(era5_mslp_path)\n",
    "era5_mslp_path.is_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a1303b-698a-4e50-8576-0d8abaa1f964",
   "metadata": {},
   "outputs": [],
   "source": [
    "era5_mslp_cube = iris.load_cube(str(era5_mslp_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13410ade-518f-4a6f-9fee-54a5ad3ecb3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.coord_categorisation.add_season_number(era5_mslp_cube, 'time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "865f1402-a075-4e33-a52e-21a8fba4eb4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "era5_mslp_cube"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad6a2e5-e655-4e3c-a287-ada60354c4d5",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Preprocessing features\n",
    "\n",
    "In this case, we previously did some preprocessing from the raw ERA5 data to prepare for our purposes. So far we have:\n",
    "* Load MLSP data (hourly, global)\n",
    "* Extract UK area from global ERA5 data\n",
    "\n",
    "Next up we want to remove the seasonal averages, so that we can get season independent clusters of weather type which will be applicable all year round.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "930757dc-5211-45de-81f2-f4f8ab5bf243",
   "metadata": {},
   "outputs": [],
   "source": [
    "season_average = [era5_mslp_cube.extract(iris.Constraint(season_number=sn1)).collapsed(['time'],iris.analysis.MEAN).data for sn1 in range(4)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188fb847-a244-486f-af9a-5edbefcb2217",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "era5_flat_deseasoned = numpy.concatenate(\n",
    "    [(era5_mslp_cube.extract(iris.Constraint(season_number=sn1)).data - season_average[sn1]).reshape(\n",
    "    (-1, era5_mslp_cube.shape[1] * era5_mslp_cube.shape[2])) for sn1 in range(4)],\n",
    "    axis=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f877566-1e59-49a0-83e9-e4d1112e1dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "era5_mslp_cube.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24837a09-878b-432e-b956-79135eb4cb57",
   "metadata": {},
   "outputs": [],
   "source": [
    "era5_flat_deseasoned.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6230499-5954-4050-a7a6-a9db62efce8b",
   "metadata": {},
   "source": [
    "### Setup and train the algorithm\n",
    "Now that we have our data, we want to do some clustering. Scikit learn give a [good overview to clustering algorithms](https://scikit-learn.org/stable/modules/clustering.html), though tis covers only what is available in sklearn many other more advanced lagorithms and implementations are available.\n",
    "\n",
    "We will use the basic [K-means algorithm](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html#sklearn.cluster.KMeans). This algorithm works as follow:\n",
    "1. randomly select n points, where n is the desired number of clusters (a hyperparameter)\n",
    "2. initiate clusters with each of the selected points the centre of a cluster\n",
    "3. for each point in the dataset, find the closest cluster centre and assign to that cluster\n",
    "4. for each of the clusters, find the centroid of all the points assigned to that cluster (or the nearest actual point to the centroid).\n",
    "5. assign those points as the new cluster centres, and repeat from point 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c3657a-7d76-425f-a465-f9670f4548f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_clusters = 10\n",
    "max_iter=1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb500fb-1154-4eb6-b583-846cab10d9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = sklearn.cluster.KMeans(n_clusters=num_clusters, random_state=0, max_iter=max_iter)\n",
    "kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d7e195-f26f-40ea-b6d1-e404e0dd5e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "kmeans.fit(era5_flat_deseasoned)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ce65ba-4a83-449b-acc4-78f56f90dcc9",
   "metadata": {},
   "source": [
    "### Inference and Evaluation\n",
    "In this case, we don't have a target to compare our results to\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f596317c-670d-4255-aaea-258b9f730bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_coord = iris.coords.DimCoord(list(range(10)),\n",
    "                                     var_name='cluster_number')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de77c866-1b2c-4850-80f9-51d7600ddc8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_centres = iris.cube.Cube(\n",
    "    data=kmeans.cluster_centers_.reshape((10,era5_mslp_cube.shape[1],era5_mslp_cube.shape[2],)),\n",
    "    dim_coords_and_dims = [(cluster_coord, 0), (era5_mslp_cube.coord('latitude'),1), (era5_mslp_cube.coord('longitude'),2)],\n",
    "    units=era5_mslp_cube.units,\n",
    "    var_name='cluster_mean_sea_level_pressure',\n",
    ")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8f42c0-a5bd-4a2d-90b3-16c159ab2089",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1 = matplotlib.pyplot.figure(figsize=(16,80))\n",
    "for ix1 in range(num_clusters):\n",
    "    ax1 = fig1.add_subplot(num_clusters,1,ix1+1,projection=cartopy.crs.PlateCarree())\n",
    "    iris.quickplot.contourf(cluster_centres[ix1,:,:],axes=ax1)\n",
    "    ax1.coastlines()    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d4ec97-1051-4998-987f-0809b796bc9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "season_clusters = [kmeans.predict(\n",
    "    (era5_mslp_cube.extract(iris.Constraint(season_number=sn1)).data - season_average[sn1]).reshape(\n",
    "    (-1, era5_mslp_cube.shape[1] * era5_mslp_cube.shape[2])) \n",
    ") for sn1 in range(4)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b9d148-3350-45c9-b4e0-b3db045a4dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "season_names =['dfj (nh winter)', 'mam (nh spring)', 'jja (nh summer)','son (nh autumn)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23f2f86-9951-4b69-80e2-9de1ea3e64d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1 = matplotlib.pyplot.figure(figsize=(16,16))\n",
    "for sn1, current_season in enumerate(season_clusters):\n",
    "    ax1 = fig1.add_subplot(2,2,sn1+1, title=season_names[sn1])\n",
    "    pandas.Series(current_season).plot.hist(ax=ax1)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec98fa0-2c3d-41cb-921c-558ff2e0d3bb",
   "metadata": {},
   "source": [
    "### Another apporach - dimensionality reduction\n",
    "\n",
    "To improve clustering reasults, there are various ways of improving\n",
    "* change how to describe clusters\n",
    "* change how to assign cluster membership\n",
    "  * change the distance metric between points\n",
    "* transform the points into a different space, whcih may preseve some properties and change others, hopefuly to improve separation.  \n",
    "\n",
    "We will demonstrate this briefly by performing *pincipal component analysis* or *PCA*  on the data to reduce the dimensionality. This will also speed up calculation.\n",
    "\n",
    "Scikit Learn reference:\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8313b982-5d2c-4591-baac-9f5f83342624",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "pca_mslp = sklearn.decomposition.PCA(n_components=50, svd_solver='full')\n",
    "pca_mslp.fit(era5_flat_deseasoned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "073fad27-e818-4726-95b8-5c930e1d0a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "era5_dim_reduced = pca_mslp.transform(era5_flat_deseasoned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe78ec65-ec7b-4a38-8eac-9c98691f81b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "kmeans_mslp_reduced = sklearn.cluster.KMeans(n_clusters=num_clusters, random_state=0, max_iter=max_iter)\n",
    "kmeans_mslp_reduced.fit(era5_dim_reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ebf7f88-dcee-4008-8666-62c44c7e2280",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_centres_pca = iris.cube.Cube(\n",
    "    data=(pca_mslp.inverse_transform(kmeans_mslp_reduced.cluster_centers_)).reshape((10,era5_mslp_cube.shape[1],era5_mslp_cube.shape[2],)),\n",
    "    dim_coords_and_dims = [(cluster_coord, 0), (era5_mslp_cube.coord('latitude'),1), (era5_mslp_cube.coord('longitude'),2)],\n",
    "    units=era5_mslp_cube.units,\n",
    "    var_name='cluster_mean_sea_level_pressure',\n",
    ")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f71bd7c5-a7d8-4acf-84a1-21adbdbfb6f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1 = matplotlib.pyplot.figure(figsize=(16,80))\n",
    "for ix1 in range(num_clusters):\n",
    "    ax1 = fig1.add_subplot(num_clusters,1,ix1+1,projection=cartopy.crs.PlateCarree())\n",
    "    iris.quickplot.contourf(cluster_centres_pca[ix1,:,:],axes=ax1)\n",
    "    ax1.coastlines()    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c3107ab-0529-47da-8d13-be6b271a38a5",
   "metadata": {},
   "source": [
    "Now to calculate the clusters for each point we perform the following:\n",
    "* remove the seasonal mean\n",
    "* reshape the data\n",
    "* apply PCA transform to reduce dimensionality\n",
    "* predict which cluster each point belongs to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d64e49cd-16b9-4db9-9552-3bd4d375882d",
   "metadata": {},
   "outputs": [],
   "source": [
    "season_clusters_pca = [kmeans_mslp_reduced.predict( pca_mslp.transform(\n",
    "    (era5_mslp_cube.extract(iris.Constraint(season_number=sn1)).data - season_average[sn1]).reshape(\n",
    "    (-1, era5_mslp_cube.shape[1] * era5_mslp_cube.shape[2]))) \n",
    ") for sn1 in range(4)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501b0de1-bd01-4d73-a3a4-05f4cb684b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1 = matplotlib.pyplot.figure(figsize=(16,16))\n",
    "for sn1, current_season in enumerate(season_clusters_pca):\n",
    "    ax1 = fig1.add_subplot(2,2,sn1+1, title=season_names[sn1])\n",
    "    pandas.Series(current_season).plot.hist(ax=ax1)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7089a67-dde5-4531-918c-fd58904002aa",
   "metadata": {},
   "source": [
    "### Saving the model\n",
    "\n",
    "Here we could very easily save the key parts of the steps and recreate them, to predict clusters on other data using our training outputs.\n",
    "* The seasonal means\n",
    "* The cluster centres\n",
    "* The PCA transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c890e74a-9009-4cc9-8237-01225a3d004e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_mslp.singular_values_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ad8947-6371-4622-98cf-ee208c91bdb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_mslp.get_covariance().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98329ef7-c77f-4838-9faa-562127517244",
   "metadata": {},
   "source": [
    "## Problem 3 - Precipitaton prediction (Regression)\n",
    "\n",
    "Now we will look at a pipeline for predicting precipitation, based on the *Precipitation Rediagnosis* dataset.\n",
    "\n",
    "This code is taken from the [repository for the Preciptation Rediagnosis project](https://github.com/informatics-lab/precip_rediagnosis/).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f70d63-f4b4-44b3-8265-46ebdebecf73",
   "metadata": {},
   "outputs": [],
   "source": [
    "bands = {\n",
    "    '0.0':[0, 0.01],\n",
    "    '0.25':[0.01, 0.5], \n",
    "    '2.5': [0.5, 4], \n",
    "    '7.0':[4, 10], \n",
    "    '10.0':[10,220]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ceda84b-a52b-43a8-89dd-7bb7932771c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "intensity_band_template = '{source}_fraction_in_band_instant_{band_centre}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48422752-8ad9-4728-a2eb-ab7462e5463c",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_parameter = [intensity_band_template.format(source='radar', band_centre=threshold) for threshold in bands.keys()]\n",
    "nwp_comparison = [intensity_band_template.format(source='mogrepsg', band_centre=threshold) for threshold in bands.keys()]\n",
    "\n",
    "profile_features = ['air_temperature', 'relative_humidity', 'wind_speed', 'wind_from_direction', 'cloud_volume_fraction'] #'air_pressure',\n",
    "single_lvl_features = []#['surface_altitude']#'thickness_of_rainfall_amount', 'surface_altitude', 'air_pressure_at_sea_level', 'cloud_area_fraction'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c98707-28f9-4932-a445-1df4f3b4b8ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_dict = {\n",
    "    'profile': profile_features,\n",
    "    'single_level': single_lvl_features,\n",
    "    'target': target_parameter,\n",
    "    'nwp': nwp_comparison, \n",
    "    'metadata': ['time', 'realization', 'latitude', 'longitude']\n",
    "} \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c376f86-ccb1-4f98-b367-cdee8797be40",
   "metadata": {},
   "source": [
    "### Data Loading & Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29ec5ff-f810-4108-9dd1-56e593d57a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    prd_data_dir = pathlib.Path(os.environ['ML_TUTORIAL_DIR']) / 'prd'\n",
    "except KeyError:\n",
    "    prd_data_dir = pathlib.Path('/project/informatics_lab/precip_rediagnosis/train_202212')\n",
    "print(prd_data_dir.is_dir())\n",
    "prd_data_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a889f47-debd-444d-9b29-d6eda287fbf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "prd_prefix_str = 'prd_merged'\n",
    "prd_csv_suffix = 'csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f9acf9-fa53-488c-a77e-6ac0c8f0aed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "prd_path_list = [p1 for p1 in prd_data_dir.rglob(f'{prd_prefix_str}*{prd_csv_suffix}')]\n",
    "prd_path_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4072db62-699c-43e9-a10b-2fecc0e7c596",
   "metadata": {},
   "source": [
    "We will use two of the scenarios for training and reserve one scenario for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b94996f-3ddf-4d86-a93d-f3f9bd2e6553",
   "metadata": {},
   "outputs": [],
   "source": [
    "prd_train_df = pandas.concat([pandas.read_csv(p1) for p1 in prd_path_list[:-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18f5f14-0655-46f8-9e87-ce67d4eb6597",
   "metadata": {},
   "outputs": [],
   "source": [
    "prd_train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adadb474-6f33-42d8-8655-cb3c05ba39b9",
   "metadata": {},
   "source": [
    "### Feauture Engineering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12085581-09a5-4f20-aca7-9f8e66a8acf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "prd_input_feature_names  = [c1 for c1 in prd_train_df.columns if 'temp' in c1] + [c1 for c1 in prd_train_df.columns if 'humid' in c1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d227919c-b1e5-4cba-86f2-6f20d1fd2d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "prd_target_feature_name = 'rainfall_rate'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0282589d-bce9-4e4d-a15f-1fabcdc8da50",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prd_train_df[prd_input_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793a4cad-6f66-4390-a815-ac7bb71f78e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "prd_train_df[prd_target_feature_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d58d1de-3842-4170-926e-0356ac66da58",
   "metadata": {},
   "source": [
    "### Data Preparation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d65c8b-b561-4a3c-a2ec-0a98c3ceb173",
   "metadata": {},
   "outputs": [],
   "source": [
    "prd_preproc_dict = {}\n",
    "for if1 in prd_input_feature_names:\n",
    "    scaler1 = sklearn.preprocessing.StandardScaler()\n",
    "    scaler1.fit(prd_train_df[[if1]])\n",
    "    prd_preproc_dict[if1] = scaler1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3e6928-56f0-4a39-a5e5-8da7c986edf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "prd_target_scaler = sklearn.preprocessing.MinMaxScaler()\n",
    "prd_target_scaler.fit(prd_train_df[[prd_target_feature_name]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d3aa1d4-4899-4c0a-bc0c-6afd61ca2864",
   "metadata": {},
   "outputs": [],
   "source": [
    "prd_X_train = preproc_input(prd_train_df, prd_preproc_dict)\n",
    "prd_y_train = prd_target_scaler.transform(prd_train_df[[prd_target_feature_name]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0400b9c9-3e75-4ba4-aba0-e448dcf0805d",
   "metadata": {},
   "outputs": [],
   "source": [
    "prd_X_train.shape, prd_y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bde11fd-a13e-4202-af48-39f0a182dcae",
   "metadata": {},
   "outputs": [],
   "source": [
    "prd_test_df = pandas.concat([pandas.read_csv(p1) for p1 in prd_path_list[-1:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de67be97-227e-415b-8db6-4f96f5bb9abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalise temp and humidity features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a687727-447f-4450-a291-d2bfd6162564",
   "metadata": {},
   "outputs": [],
   "source": [
    "prd_test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34fd69b-cd02-43e8-a5b9-f34e82e687e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "prd_X_test = preproc_input(prd_test_df, prd_preproc_dict)\n",
    "prd_y_test = prd_target_scaler.transform(prd_test_df[[prd_target_feature_name]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e5bd30-52a6-4be1-9354-23763f064f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "prd_X_test.shape, prd_y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b34712-9249-4961-aad4-130b3668af41",
   "metadata": {},
   "outputs": [],
   "source": [
    "prd_tuples = [\n",
    "    (prd_X_train, prd_y_train),\n",
    "    (prd_X_test, prd_y_test),    \n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30268b69-e91d-4c19-a7e0-a43cd6100ddf",
   "metadata": {},
   "source": [
    "### Algorithm Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "366e2c7f-b4b6-48bf-bc40-76ba5105d3b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "prd_classifiers_params = {\n",
    "    'random_forest': {'class': sklearn.ensemble.RandomForestRegressor, 'opts': {'max_depth':10, }},\n",
    "     'ann_5_500': {'class': sklearn.neural_network.MLPRegressor, 'opts': {'hidden_layer_sizes':(500,500,500,500,500)}},   \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a155477c-0610-4bc1-89a1-cca046e45ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "prd_classifiers_dict = {}             \n",
    "for clf_name, clf_params in prd_classifiers_params.items():\n",
    "    clf1 = clf_params['class'](**clf_params['opts'])\n",
    "    prd_classifiers_dict[clf_name] = clf1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "addc5df4-1872-4878-97e4-53780ae1ae93",
   "metadata": {},
   "source": [
    "## Algorithm Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd7cff3-ec59-486e-b56d-cff79a0ac646",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_training_samples = 50000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "744efcf2-81d3-4882-b91b-96f14f6d5d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for clf_name, clf1 in prd_classifiers_dict.items():\n",
    "    print(clf_name)\n",
    "    clf1.fit(prd_X_train[:num_training_samples,:], prd_y_train[:num_training_samples])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff070c72-3534-4efe-9802-57bb0134581b",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be510fa5-e8fc-4eda-83cc-11b1fd00514b",
   "metadata": {},
   "outputs": [],
   "source": [
    "prd_y_pred_train = {}\n",
    "\n",
    "for clf_name, clf1 in prd_classifiers_dict.items():\n",
    "    prd_y_pred = clf1.predict(prd_X_train)\n",
    "    prd_y_pred_train[clf_name] = prd_y_pred\n",
    "    prd_train_df[f'pred_{clf_name}'] = prd_y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca28c708-3739-4e8e-bed2-6150e0f9aec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "prd_y_pred_test= {}\n",
    "\n",
    "for clf_name, clf1 in prd_classifiers_dict.items():\n",
    "    prd_y_pred = clf1.predict(prd_X_test)\n",
    "    prd_y_pred_test[clf_name] = prd_y_pred\n",
    "    prd_test_df[f'pred_{clf_name}'] = prd_y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28097488-15b9-4c62-a9f3-c3208583fb62",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6e4c37-3bfc-4051-a54c-530fda5aee35",
   "metadata": {},
   "outputs": [],
   "source": [
    "prd_test_rmse = sklearn.metrics.mean_squared_error(\n",
    "    prd_y_test,\n",
    "    prd_classifiers_dict['ann_5_500'].predict(prd_X_test))\n",
    "prd_test_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8aa2c0e-d74c-420f-9ee9-23b1857bb5a3",
   "metadata": {},
   "source": [
    "### Caveats for this example\n",
    "\n",
    "This is a very naive solution to this problem, and it is giving us bad results. This is not a surprise, as precipitation is generally considered a very difficult field to predict using any we will look at a more sophisticated approach in the next notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df1260a0-d80a-4566-a9bc-e9867904b6be",
   "metadata": {},
   "source": [
    "## Examples of use\n",
    "* You can see more example notebook relating to this challenge on the [Data Science CoP GitHub repository](https://github.com/MetOffice/data_science_cop/tree/master/challenges/2021_falklands_rotors).\n",
    "\n",
    "More Information on Weather Patterns can be found here, which the full much more sophisticated technique used in the real application:\n",
    "* [Met Office Info Page on Weather Types](https://www.metoffice.gov.uk/research/news/2016/new-weather-patterns-for-uk-and-europe)\n",
    "* [Paper by Neal, Fereday et al.](https://rmets.onlinelibrary.wiley.com/doi/full/10.1002/met.1563)\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df3187a-261f-4bed-ae13-9b11f47a5f77",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Next steps\n",
    "\n",
    "* [Rotors Challenge Notebooks](https://github.com/MetOffice/data_science_cop/tree/master/challenges/2021_falklands_rotors) \n",
    "* [Leeds University Notebooks](https://cemac.github.io/LIFD_ENV_ML_NOTEBOOKS/) \n",
    "* [Kaggle Weather Types Clustering Competition](https://www.kaggle.com/code/prakharrathi25/weather-data-clustering-using-k-means/notebook)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a99adc-b92c-44fe-917f-b0ab2d9552d4",
   "metadata": {},
   "source": [
    "## Dataset Info\n",
    "\n",
    "### Falklands Rotors Challenge Dataset\n",
    "Crown Copyright 2021 - This dataset was created by Met Office Chief Operational Meterologist Steve Ramsdale from Met Office forecast and observation data.\n",
    "* Model Data - Met Office Global 10km resolution model\n",
    "* Observations - made by meteorologists at Mount Pleasant airfield in the Falkland Islands.\n",
    "\n",
    "### ERA5\n",
    "ERA5 is Renanlysis data created by ECMWF. Reanalysis combines observations from many sources. by assimilating these into a forecast model (ECMWF's IFS in this case), to provide a consistent physically valid gridded dataset that is a close to observations as possible\n",
    "https://www.ecmwf.int/en/forecasts/datasets/reanalysis-datasets/era5\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e69e18e-5fe4-41c6-8380-52719b9f082d",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "The format of this notebook is based on the [template for tutorial notebooks](https://github.com/geo-yrao/notebook-dev/blob/main/templates/NCAI_Training_Notebook_template%20-%20Distribution%20Copy.ipynb) developed by NOAA, available on GitHub."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".conda-ml-weather-tutorial-skl Python (Conda)",
   "language": "python",
   "name": "conda-env-.conda-ml-weather-tutorial-skl-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
