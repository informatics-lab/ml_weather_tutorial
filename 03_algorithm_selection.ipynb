{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e6d58f1-20fc-4368-93d7-11f47cdd2cef",
   "metadata": {},
   "source": [
    "# Algorithm & Metric Deep Dive - Making appropriate choices for your problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb798bbe-e4a4-4da9-9539-3a4065ca6572",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Overview \n",
    "\n",
    "In the previous notebook, we built a machine learning pipeline containing all the important elements we would find in any sophisticated, real world pipeline, but in our example we used quite simple components for each step. Even still, there were quite a number of *hyperparameters* we had to choose along the way, without having understood what we were choosing and why, or more importantly, understanding what would be a good choice for our particular problem and the dataset. This notebook will attempt to give a little insight into how key ML algorithms work with the aim of giving some basic understanding of what the hyperparameters mean and how they influence the end result. This will hopefully be a starting point for making appropriate choices of algorithm, hyperparameters and metrics. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a174fd29-1d97-435d-b5df-f7afbc70a7d2",
   "metadata": {},
   "source": [
    "### Prerequisites\n",
    "* Completed notebooks 1 & 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3db6639-c81c-438b-a792-f4ed8ca27f0f",
   "metadata": {},
   "source": [
    "### Learning Outcomes \n",
    "\n",
    "* Understand mechanisms of key tree based and neural network algorithms\n",
    "* Understand key hyperparameters and how to choose them\n",
    "* Understand key metrics and how to select the right one for your problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40af0dd1-fc66-4b0f-8f29-ca2a8568c02a",
   "metadata": {},
   "source": [
    "### Links to Best practices and Values\n",
    "* ML Pitfalls - Avoid problems such as overfitting and underfitting through appropriate choice of algorithms and hyperparameters\n",
    "* Ethics - Be able to justify your choices for what you have implemented\n",
    "* ML Lifecycle - Ensure you are able to reproduce results\n",
    "\n",
    "(Please see [Met Office Best Prctice Summary document](https://metoffice.sharepoint.com/:w:/r/sites/MetOfficeDataScienceCommunity/Shared%20Documents/MLAG/project_evaluation_review/MachineLearningBestPracticesAndValues_v1_0.docx?d=wae2554217d0342f2a9c8cf8906bfbe38&csf=1&web=1&e=ja7rLn) on Data Science Community site (internal only) for more information and further reading.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee931066-ee2a-447b-bbdb-267fbeeb4e47",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Data Science Framework\n",
    "* Radiation emulation - Fusing Simulation and Data Science\n",
    "* XBT - Uncertainty and Trust\n",
    "* Rotors - Data to Decisions\n",
    "* Weather regimes - Discovery and Attribution\n",
    "\n",
    "Further Reading\n",
    "* [Data Science Framework](https://www.metoffice.gov.uk/research/foundation/informatics-lab/met-office-data-science-framework)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f980f2-1d40-41d2-974c-aef97592e7af",
   "metadata": {},
   "source": [
    "## Tutorial - Decision Trees\n",
    "\n",
    "The first algorithm we will look at in detail is the family of algorithms based on decision trees. A decision tree is basically a flow chart where at each node a question is asked of the data and you proceed along one branch until you run out of nodes, you are then presented with an answer for the end point in the flowchart that you have reached. Below we have an example of such a \"manually created\" decision tree, where expert judgement has informed the questions of the data. In this case, metadata around a set of ocean observations is being queried, to be able to infer what sort of probe was used to make the measurement. So a question being asked in the flowchart is \"Was the measurement made by a certain set of countries\" or \"Was the maximum depth of the measurement within one of several specified range\"? Following the flowchart to the end we get an answer about which type of probe the expert thinks that measurement has come from. In decision trees, a training algorithm constructs such a series of questions from the data.\n",
    "\n",
    "![An example of manually created flowchart for making decisions, similar to the structure of the decision tree.](xbt_imeta_flowchart.png)\n",
    "\n",
    "To start with lets explain some key terminology for decision trees:\n",
    "* *Root node* - The base node of the decision tree.\n",
    "* *Splitting* - The process of dividing a node into multiple sub-nodes.\n",
    "* *Decision node* - When a sub-node is further split into additional sub-nodes.\n",
    "* *Leaf node* - When a sub-node does not further split into additional sub-nodes; represents possible outcomes.\n",
    "* *Pruning* - The process of removing sub-nodes of a decision tree.\n",
    "* *Branch* - A subsection of the decision tree consisting of multiple nodes.\n",
    "* *Hyperparameters*\n",
    "  * *Max depth* -  The maximum number of levels in the tree, i.e. the maximum number of decision nodes before an answer is reached.\n",
    "  * *Minimum leaf node samples* - The minimum number of data samples in the training dataset that can be assigned to a leaf node, if splitting would result in fewer data points for the leaf node, then splitting will not occur.\n",
    "  * **\n",
    "![Image of tree](https://miro.medium.com/max/1400/1*3P1333UmqEww6YMpjisj4Q.png)\n",
    "\n",
    "(from article https://towardsdatascience.com/decision-trees-explained-3ec41632ceb6 )\n",
    "\n",
    "### Training a decision tree\n",
    "\n",
    "The decision tree is constructed with an algorithm that recursively splits nodes based on a criteria until some or other constraints are reached, such as max depth or min leaf sample hyperparameters. In the decision tree, the nodes are split into sub-nodes on the basis of a threshold value of an attribute, considering one node and one attribute at a time. ([Source of quote](https://www.analyticssteps.com/blogs/classification-and-regression-tree-cart-algorithm) )\n",
    "\n",
    "The procedure is approximately as follows\n",
    "\n",
    "* Start with all training data points in the root node, which is also a leaf node to begin with.\n",
    "* Go through current leaf nodes considering  whether to split into a decision node and 2 leaf nodes.\n",
    "* At each leaf node, consider each of the features in turn e.g. year, max depth, country\n",
    "* For categorical features, consider each of the potential values. \n",
    "* For continuous variable, find an appropriate threshold value on which to split the data\n",
    "* Calculate the value of the information metric for each potential split e.g. entropy, Gini Index. \n",
    "* The node split that provides the biggest increase or decrease (depending on the specific metric) in the value at that node is chosen as the branching criteria for a new decision node to replace the leaf node. The CART algorithm does that by searching for the best homogeneity for the sub-nodes, with the help of the Gini Index criterion. \n",
    "* The data points at this leaf node are divided between two new leaf nodes, for one of the outcomes of the decision criteria (e.g. max_depth < 400m or max depth >=- 400m).\n",
    "* move onto next available leaf node until constrain has been reached.\n",
    "\n",
    "More information on the details of the Maths behind training a decision tree is are available here: [Maths of CART algorithm](https://towardsdatascience.com/the-mathematics-of-decision-trees-random-forest-and-feature-importance-in-scikit-learn-and-spark-f2861df67e3)\n",
    "\n",
    "Key training terms\n",
    "* Greedy algorithm -  an algorithm that consider leaf nodes one at a time, rather than optimising globally.\n",
    "* Stopping criteria - The criteria for stopping the recursive splitting of nodes, for example max tree depth\n",
    "* Pruning - A complimentary process of cutting off branches of the decision to simplify it. Sophisticated algorithms can cycle between splitting and pruning operations.\n",
    "\n",
    "### Pros and Cons of Decision Trees\n",
    "\n",
    "Advantages:\n",
    "* Works for numerical or categorical data and variables.\n",
    "* Models problems with multiple outputs.\n",
    "* Requires less data cleaning than other data modeling techniques. \n",
    "* Easy to explain to those without an analytical background. \n",
    "\n",
    "Disadvantages: \n",
    "* Affected by noise in the data.\n",
    "* Not ideal for large datasets.\n",
    "* Can disproportionately value, or weigh, attributes.\n",
    "* The decisions at nodes are limited to binary outcomes, reducing the complexity that the tree can handle. Trees can become very complex when dealing with uncertainty and numerous linked outcomes. \n",
    "\n",
    "\n",
    "### Variations on a decision tree\n",
    "\n",
    "While standard decision trees are efficient and powerful tools, as noted above the do have limitations and flaws. Variations on these have been developed to overcome some of these limitations.\n",
    "\n",
    "* *Random Forests* are ensembles of decision trees where each tree in the forest is trained on different subset of the data (bootstrapping) and subset of the features (bagging). In a classification problem, each tree votes for a classification and the result is the one with the most votes.\n",
    "* *Gradient boosted trees* use a more complicated method for joining together the output of the ensemble of decision trees, explicitly modeling a loss function and the gradient of the loss function. The gradient boosting is applied to this function, together with the random selection of input data and features used in random forests. For more info on the technical details, please see the links below.\n",
    "* Xgboost - Standing for eXtreme-Gradient-boosted trees,  this is the current state of the art in ensemble tree-based methods.\n",
    "Random forest discussion \n",
    "Deals with variance Leo breiman\n",
    "Key terms\n",
    "* Bagging - Selecting a subset of the input features for training each decision tree in an ensemble.\n",
    "* Bootstrapping - Selecting a random subset of the data points to separately train each decision tree in an ensemble.\n",
    "* Aggregation - Bringing together the results of an ensemble to produce a single classification or regression output/ result.\n",
    "* Ensemble model - A model that joins many lower-performance models together to achieve better performance of the ensemble. This can also be used to determine probabilistic  (or pseudo-probabilistic) information from the ensemble of classifiers.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "949ea8c6-2ead-41b3-b7c3-71b946891c9e",
   "metadata": {},
   "source": [
    "# Example: Decision Trees - XBT classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e73c9103-8a88-4bf5-a6fd-48a05597b9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import os\n",
    "import functools\n",
    "import math\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73b0314-cc10-44ea-8409-1aefe5b1ef56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e77bb0-1ae3-478b-bcf3-463381f6d023",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4870b309-b6f9-4a81-9d44-984d2af0771a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import sklearn.tree\n",
    "import sklearn.preprocessing\n",
    "import sklearn.ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c78392c-694f-49a1-9347-35983ad1c0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "xbt_data_loc = pathlib.Path('/project/informatics_lab/xbt')\n",
    "xbt_fname_template = 'xbt_{year}.csv'\n",
    "year_range= (1966,2015)\n",
    "xbt_df = pandas.concat([pandas.read_csv(xbt_data_loc / xbt_fname_template.format(year=year1)) for year1 in range(year_range[0], year_range[1])])\n",
    "xbt_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f4ae3a2-3b22-4c8e-b787-5be72ccbde95",
   "metadata": {},
   "source": [
    "#### Clean dataset\n",
    "Remove bad data points from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f954082e-ae95-45f3-aaf3-93881f5e4d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "xbt_df = xbt_df[~((xbt_df['max_depth'] < 0) | (xbt_df['max_depth'].isna()))]\n",
    "xbt_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d17d54f6-ebcd-4622-86cf-34493d6d38f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_feature = 'instrument'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f69d970-ae1b-4e37-ad7c-3e4aad3e990a",
   "metadata": {},
   "outputs": [],
   "source": [
    "xbt_df[target_feature].value_counts().index[:12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96275645-9a21-4419-81f3-664342fa6c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "xbt_df = xbt_df[~(xbt_df[target_feature].str.contains('UNKNOWN'))]\n",
    "xbt_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d1551f1-9a1d-44d0-8f36-f905cec21ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "xbt_df = xbt_df[xbt_df[target_feature].isin(list(xbt_df[target_feature].value_counts().index[:12]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e363bf-6d3e-451c-bd78-289a5d6416ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "xbt_labelled = xbt_df[xbt_df['imeta_applied'] == 0]\n",
    "xbt_unlabelled = xbt_df[xbt_df['imeta_applied'] != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c141ab5-f1b1-4791-b082-ce662d1514d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "xbt_labelled['instrument'].value_counts().plot.pie()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "622ffbc5-c1e6-471e-a6e4-59a9fab213d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "xbt_labelled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0142f623-20f9-421e-8ffa-252e3f982877",
   "metadata": {},
   "outputs": [],
   "source": [
    "xbt_unlabelled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b6a597d-462a-4a2c-9c23-d3304ced7d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "xbt_train, xbt_test = sklearn.model_selection.train_test_split(xbt_labelled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af060a4-ab32-48b0-8b20-372a76433306",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_dict = {\n",
    "    'year': sklearn.preprocessing.MinMaxScaler(),\n",
    "    'max_depth': sklearn.preprocessing.MinMaxScaler(),\n",
    "    'lat': sklearn.preprocessing.MinMaxScaler(),\n",
    "    'lon': sklearn.preprocessing.MinMaxScaler(),\n",
    "}\n",
    "input_features = [list(scaler_dict.keys())]\n",
    "\n",
    "preproc_input_features = []\n",
    "for feature_name, scaler1 in scaler_dict.items():\n",
    "    scaler1.fit(xbt_train[[feature_name]])\n",
    "    preproc_input_features += [scaler1.transform(xbt_train[[feature_name]])]\n",
    "    \n",
    "X_train = numpy.concatenate( preproc_input_features, axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34125b8d-4869-4de5-820e-6f184255d94b",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_encoder = sklearn.preprocessing.LabelEncoder()\n",
    "target_encoder.fit(xbt_train[target_feature])\n",
    "y_train = target_encoder.transform(xbt_train[target_feature])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b9e10eb-2a6e-432d-8027-0f1d323f3942",
   "metadata": {},
   "source": [
    "We can get the hyperparameters for our decision tree by creating a decision tree object. You can get more explanation from `help(sklearn.tree.DecisionTreeClassifier`, or from the [scikit-learn docs](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159abc6e-13bf-4abb-b8ae-ce047fb0b8c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn.tree.DecisionTreeClassifier().get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9383a5ba-3ba4-4bdd-beab-977eefefe045",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "dt_clf = sklearn.tree.DecisionTreeClassifier(\n",
    "    max_depth=5, # reduce chance of overfitting\n",
    "    min_samples_leaf= 2, #ensure that there won't be too small a number of samples in a leaf node\n",
    "    min_samples_split= 5, # ensure more sample at a node when it splits\n",
    ")\n",
    "dt_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "484bf4de-51cc-4325-a71e-4843239bab21",
   "metadata": {},
   "source": [
    "https://scikit-learn.org/stable/auto_examples/tree/plot_unveil_tree_structure.html#sphx-glr-auto-examples-tree-plot-unveil-tree-structure-py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d99691df-7b5f-4724-8415-9881f80e38ec",
   "metadata": {},
   "source": [
    "https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fab12db-3995-48ee-b65e-16dbba75a3c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1 = matplotlib.pyplot.figure(figsize=(64,64))\n",
    "_ = sklearn.tree.plot_tree(dt_clf)\n",
    "matplotlib.pyplot.show()\n",
    "fig1.savefig('treevis.svg',bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db514ea-cf80-46ed-8a01-6cd4d5ad779e",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest_hyperparameters = {\n",
    "    'max_depth' : 5, # reduce chance of overfitting\n",
    "    'min_samples_leaf' :  2, #ensure that there won't be too small a number of samples in a leaf node\n",
    "    'min_samples_split' :  5, # ensure more sample at a node when it splits\n",
    "    'n_estimators' : 20, # restrict number of trees in forest\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f647f39-580e-4e8e-89ff-f59ece58a388",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "rf_clf = sklearn.ensemble.RandomForestClassifier(\n",
    "    **random_forest_hyperparameters\n",
    ")\n",
    "rf_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea625a1-726b-4c13-a1e7-afd9b2f7b77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for p1,param_val in rf_clf.get_params().items():\n",
    "    print(f'param value {p1}={param_val}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd16e65-53c5-456e-b7c8-05e93777e504",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_clf.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29be4895-11b6-427f-8dc7-2ee20bdc7ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_clf.estimators_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c0e37b-5926-4e3d-86ce-2b2808637d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate results from random forest and decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2939c278-7680-4686-b055-808ddfb5313c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = numpy.concatenate(\n",
    "    [scaler1.transform(xbt_test[[feature_name]]) for feature_name, scaler1 in scaler_dict.items()],\n",
    "    axis=1)\n",
    "y_test = target_encoder.transform(xbt_test[target_feature])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1ae1e4-f7cd-491b-a7c7-54c7f254f885",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_dt = dt_clf.predict(X_test)\n",
    "y_pred_rf = rf_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cfc8863-e1c6-406f-ac14-6298924dd1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1 = matplotlib.pyplot.figure(figsize=(16,8))\n",
    "ax1 = fig1.add_subplot(1,2,1, title='frequency of different labels for decision tree predictions.')\n",
    "pandas.Series(target_encoder.inverse_transform(y_pred_dt)).value_counts().plot.pie(ax=ax1)\n",
    "ax1 = fig1.add_subplot(1,2,2, title='frequency of different labels for random forest predictions.')\n",
    "pandas.Series(target_encoder.inverse_transform(y_pred_rf)).value_counts().plot.pie(ax=ax1)\n",
    "                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2df379-3683-481c-a6d8-fd8f3e858d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list(target_encoder.classes_)), len(list(sklearn.metrics.precision_score(y_test, y_pred_dt, average=None)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe00114-ca18-4366-91c3-b119330e245c",
   "metadata": {},
   "outputs": [],
   "source": [
    "xbt_train['instrument'].unique().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f49b813-464f-4999-9ede-3d205fdf67c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "prec_dt, recall_dt, f1_dt, support_dt = sklearn.metrics.precision_recall_fscore_support(y_test, y_pred_dt, average=None)\n",
    "prec_rf, recall_rf, f1_rf, support_rf = sklearn.metrics.precision_recall_fscore_support(y_test, y_pred_rf, average=None)\n",
    "metrics_xbt = pandas.DataFrame({\n",
    "    'classes': list(target_encoder.classes_),\n",
    "    'precision_dt': list(prec_dt),\n",
    "    'precision_rf': list(prec_rf),\n",
    "    'recall_dt': list(recall_dt),\n",
    "    'recall_rf': list(recall_rf),\n",
    "    'f1_dt': list(f1_dt),\n",
    "    'f1_rf': list(f1_rf),\n",
    "    'support_dt': list(support_dt),\n",
    "    'support_rf': list(support_rf),  \n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c18243b-cf55-4c24-884f-fb19141d4146",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1 = matplotlib.pyplot.figure(figsize=(20,30))\n",
    "ax1 = fig1.add_subplot(2,2,1)\n",
    "metrics_xbt.plot.bar(x='classes',y=['recall_dt','recall_rf'],ax=ax1)\n",
    "ax1 = fig1.add_subplot(2,2,2)\n",
    "metrics_xbt.plot.bar(x='classes',y=['precision_dt','precision_rf'],ax=ax1)\n",
    "ax1 = fig1.add_subplot(2,2,3)\n",
    "metrics_xbt.plot.bar(x='classes',y=['f1_dt','f1_rf'],ax=ax1)\n",
    "ax1 = fig1.add_subplot(2,2,4)\n",
    "metrics_xbt.plot.bar(x='classes',y=['support_dt','support_rf'],ax=ax1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c28f9d5-68b8-46b8-896f-f5a1cfcf2075",
   "metadata": {},
   "source": [
    "## Tutorial - Neural Networks\n",
    "\n",
    "A lot of the staggering advancement in recent years in applying machine learning to complex real-world situations has come from the use of large neural networks, often referred to as \"deep learning\" for reasons that will become apparent in this tutorial. To start with lets consider the building block of the neural network: a single \"neuron\". As the name suggests, this concept in inspired by a biological neuron found in many brains, specifically human brains. I won't elaborate on the similarities and differences here, but will say that artificial neurons do not work like biological neurons, though there continues to interesting cross-pollination of ideas between study of artificial and biological neurons and how networks of them learn.\n",
    "\n",
    "A single artificial neuron (hereafter just referred to as a neuron), is essentially a linear weighted sum of inputs plus a constant term, to which a threshold operation is applied to the result, so that the output is 1 (activated) or 0 (not activated). The cell is known as a *perceptron*. The perceptron can be trained by iteratively updating the weights so that the error in the output on data, for example training data is reduced and minimised. This is usually done through a process called *gradient descent*. A perceptron can be updated to output a range between 0 and 1, rather than binary 0 or 1 output, using a Sigmoid function rather than a threshold operation. This results in what is known as a *sigmoid neuron*.\n",
    "\n",
    "Neural networks are a series of these neurons joined together in a network of layers. Initially the computational cost of updating the weights for more than a few neurons was prohibitive. As computers became more powerful, more neurons were used, with more *layers*. A layer is where the input of one set of neurons becomes the input for a subsequent neurons. How neurons are connected together is what determines *network architecture*. Initially There was one layer into which inputs where feed, then the outputs all went to 1 neuron to produce a single output. Subsequently layers were added in between which directly connected to neither input nor output, which are termed hidden layers. A simple feed-forward fully connected network is usually visualised as follows (from wikipedia):\n",
    "![A feedforward network with 1 hidden layer](images/Colored_neural_network.svg)\n",
    "\n",
    "The way gradient descent works is to calculate the change in the loss or cost function as you change each weight in turn. As the network grows and the number of weights grows with it, this quickly becomes expensive, and  numerical issues arise with training the weights to produce a good result. Training is now done through a technique called \"back-propagation\", which efficiently calculates the gradients in the the weights one layer at a time, and uses the the previously calculated gradients for each layer moving backwards to calculate the gradient for the next (hence the name back-propagation where the gradients being calculated propagate backwards like a wave.\n",
    "Further optimisations have been introduced as networks have grown, such stochastic gradient descent where subsets of training data are used in *batches* to update the weights. The mathematics around this quickly becomes very complex, so consult the references for more on the mathematical details, which are very interesting!\n",
    "\n",
    "Key terms in a neural network (NN)\n",
    "* Neuron - a computing unit consisting of a weighted combination of inputs, loosely mimicing a biological neuron found in animal brains. \n",
    "* Perceptron - A specific neuron with a linear weighted sum of inputs plus an activation threshold.\n",
    "* Activation - Where the output from a perceptron is zero or non-zero.\n",
    "* Weights - The co-efficient applied to each input to the neuron.\n",
    "* Bias - the constant term\n",
    "* Sigmoid function - A non-linear function applied to the linear weighted sum to ensure the output is in the range 0 to 1.\n",
    "\n",
    "Key terms in training a NN\n",
    "* Gradient descent - The process of updating the weights of a neural network, based on the partial derivatives of the cost function with respect to each of the weights in the network, updating the weights \"towards\" the direction of steepest descent of the cost function.\n",
    "* back propagation - the process of more efficiently calculating the cost function gradients for a network by calculating one layer at a time, and saving total computation in this way.\n",
    "* mini batch - In gradient descent, when training using stochastic gradient descent, a mini-batch is the subset of input data for which weight gradient are calculated together.\n",
    "* epoch - Several batches of processing of updating weights, which cumulatively cover the whole input dataset.\n",
    "* cost function - the different between the ground truth the network is trying to predict and the actual predictions made by the network.\n",
    "* local optimum - a place in the cost function where minor changes to parameters will only increase the cost function.\n",
    "* Learning rate - The scaling factor of the movement \"down the slope\" during gradient descent. The larger the learning rate the faster the movement of weights towards an optimum, but the greater the change of finding a local optimum.\n",
    "\n",
    "Hyperparameters\n",
    "* learning rate (see above)\n",
    "* batch size - the input of input points used for calculate a batch of gradients in stochastic gradient descent.\n",
    "* solver - The variant of gradient descent with back propagation used for training\n",
    "* maximum iterations - The total number of training loops (usually epochs) before terminating (if another stopping condition is not reached).\n",
    " \n",
    "Types of NN\n",
    "* feed forward - where predictions move forward through the network from input to output\n",
    "* Convolutional Neural Network - Where some weights are shared, in the form of convolutional kernels (like image processing). The network learns these kernels along with other weights.\n",
    "* Recurrent Neural Network - A network where some outputs feedback as input to previous layers. With careful arrangement, this allows the network to have a \"memory\" of previous input. This is used to learn data in a series or sequence, e.g. a time series variable or the words in a sentence.\n",
    "* Graphical Neural Network - A neural network structured around a graph representation of data.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a9ee1f-3c6d-4a4f-8117-a6720ed2337f",
   "metadata": {},
   "source": [
    "## Example - SOCRATES Radiation Model Emulation\n",
    "\n",
    "In our first example, we will be trying to emulate the SOCRATES radiation scheme in the Unified Model. This is a supervised regression problem, trying to emulate the very complex function represented by the radiation scheme,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b38fd4b-ef1d-4929-bc87-373030a9bac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow \n",
    "import keras\n",
    "import tensorflow.keras\n",
    "import tensorflow.keras.layers \n",
    "import tensorflow.keras.models \n",
    "import code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a80c7d2-de48-4eac-ac14-e81907c9c0b4",
   "metadata": {},
   "source": [
    "## Define inputs\n",
    "Specify the hyperparameters for the pipeline and the location of the input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d47a88f-7c9e-4148-8125-f0fb9e288531",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir   = pathlib.Path('/project/informatics_lab/data_science_cop/socrates_emulation/')\n",
    "output_dir = pathlib.Path(os.environ['SCRATCH']) / 'ml_weather_tutorial'\n",
    "\n",
    "if not output_dir.is_dir():\n",
    "    output_dir.mkdir()\n",
    "    print(f'creating directory {output_dir}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2121ef05-b41c-4c83-a950-86b99a8e662c",
   "metadata": {},
   "source": [
    "Set up the hyperparameters for training the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb16076-85ab-49e9-9a4e-98d437da5d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "wl='sw'\n",
    "target='nflx' # net flux, or net divergence (ndiv), which interacts with model and is used to calculate heating rate increments.\n",
    "nsamps = '50.0K'  # randomly sampled from train / test sets.\n",
    "# try 100K and 200K\n",
    "scale_data = True\n",
    "if wl=='sw':\n",
    "  model = 'sw_260'\n",
    "  model_ref = 'sw_ga7'\n",
    "elif wl=='lw':\n",
    "  model = 'lw_300'\n",
    "  model_ref = 'lw_ga7'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71dec71a-599e-41a2-b200-9cd6f6157970",
   "metadata": {},
   "source": [
    "Construct the paths to file names that contain the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1612a73b-7e99-43e9-a09b-72d2d78184df",
   "metadata": {},
   "outputs": [],
   "source": [
    "fnext='train'\n",
    "fn_meta = model+'_meta_'+nsamps+'_'+fnext+'.npz'\n",
    "fn_dat_levs = model+'_dat_levs_'+nsamps+'_'+fnext+'.npz'\n",
    "fn_dat_lays = model+'_dat_lays_'+nsamps+'_'+fnext+'.npz'\n",
    "fn_dat_surf = model+'_dat_surf_'+nsamps+'_'+fnext+'.npz'\n",
    "if target=='nflx':\n",
    "  fn_trg = model+'_trg_levs_'+nsamps+'_'+fnext+'.npz'\n",
    "if target=='ndiv':\n",
    "  fn_trg = model+'_trg_lays_'+nsamps+'_'+fnext+'.npz'\n",
    " \n",
    "fnext='test'\n",
    "fn_meta_test = model+'_meta_'+nsamps+'_'+fnext+'.npz'\n",
    "fn_dat_levs_test = model+'_dat_levs_'+nsamps+'_'+fnext+'.npz'\n",
    "fn_dat_lays_test = model+'_dat_lays_'+nsamps+'_'+fnext+'.npz'\n",
    "fn_dat_surf_test = model+'_dat_surf_'+nsamps+'_'+fnext+'.npz'\n",
    "if target=='nflx':\n",
    "  fn_trg_test = model+'_trg_levs_'+nsamps+'_'+fnext+'.npz'\n",
    "  fn_trg_ref = model_ref+'_trg_levs_'+nsamps+'_'+fnext+'.npz'\n",
    "if target=='ndiv':\n",
    "  fn_trg_test = model+'_trg_lays_'+nsamps+'_'+fnext+'.npz'\n",
    "  fn_trg_ref = model_ref+'_trg_lays_'+nsamps+'_'+fnext+'.npz'\n",
    "\n",
    "print('root dir:',data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ecd226-2fc8-4ad2-b90c-817c1fefc9c0",
   "metadata": {},
   "source": [
    "### Loading and preparing the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a1a64f-dd58-4400-9b51-896f17f22bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('loading',fn_dat_lays)\n",
    "with numpy.load(data_dir / fn_dat_lays) as npzfile:\n",
    "    dat_lays = npzfile['dat_lays']\n",
    "    \n",
    "print('loading',fn_dat_surf)\n",
    "with numpy.load(data_dir / fn_dat_surf) as npzfile:\n",
    "    dat_surf = npzfile['dat_surf']\n",
    "    \n",
    "print('loading',fn_trg)\n",
    "with numpy.load(data_dir / fn_trg) as npzfile:\n",
    "    if target=='nflx':\n",
    "         trg = npzfile['trg_levs']\n",
    "    elif target=='ndiv':\n",
    "         trg = npzfile['trg_lays']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463356e2-60f7-40cf-aec5-23eb74a05b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "nsamps = trg.shape[0]\n",
    "nlays = dat_lays.shape[1]\n",
    "nlay_feats = dat_lays.shape[2]\n",
    "nsurf_feats = dat_surf.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b49719-c261-4099-9352-3ce4ac9b28fe",
   "metadata": {},
   "source": [
    "### Single level features \n",
    "\n",
    "Some of the feature are single level features, so there is one scalar value per column. These features include:\n",
    "* `stoa - top most atmospheric radiation\n",
    "* ``surfsw` / `surflw` - surface albedo for long-wave (lw) or short-wave (sw) radiation\n",
    "* `szen` - solar zenith\n",
    "* `tstar` -temperature scaling factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84550bb4-dc40-4f5c-847a-f0a69d206470",
   "metadata": {},
   "outputs": [],
   "source": [
    "dat_surf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23198479-a9f9-4cb6-a0dc-96642e08dfaa",
   "metadata": {},
   "source": [
    "### Features on layers\n",
    "These input features have a value for each layer (a layer is a slice of the atmosphere between different model levels). \n",
    "These include various atmospheric measurements: cloud quantities, aerosols, temp, humidity and pressure,\n",
    "data taken from GA7 case study suite\n",
    "\n",
    "Layer inputs\n",
    "* accum \n",
    "* agocff\n",
    "* agsoot\n",
    "* aitken\n",
    "* biogenic\n",
    "* bioms1\n",
    "* bioms2\n",
    "* clfr\n",
    "* dustdiv1\n",
    "* dustdiv2\n",
    "* dustdiv3\n",
    "* dustdiv4\n",
    "* dustdiv5\n",
    "* dustdiv6\n",
    "* frocff\n",
    "* frsoot\n",
    "* iclfr\n",
    "* ire\n",
    "* iwm\n",
    "* lwm\n",
    "* naclflm\n",
    "* nacljet\n",
    "* o3\n",
    "* p\n",
    "* q\n",
    "* re\n",
    "* sulph\n",
    "* t\n",
    "* wclfr\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09586ce4-57c3-4b9b-9d67-a8b2aa1e8ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dat_lays.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abdd3c62-b171-4985-a472-7ec21652b642",
   "metadata": {},
   "outputs": [],
   "source": [
    "# net flux at each level, for 50k randfomly selected points\n",
    "trg.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5fb8e27-4066-4572-941a-75451ee538a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if scale_data: # normalize by range\n",
    "  scaler_lays = []\n",
    "  use_lays = []\n",
    "  for ic in range(nlay_feats):\n",
    "    min0 = numpy.min(dat_lays[:,:,ic])\n",
    "    range0 = numpy.max(dat_lays[:,:,ic]) - min0\n",
    "    if range0 > 0.:\n",
    "      dat_lays[:,:,ic] = (dat_lays[:,:,ic] - min0)/range0\n",
    "      scaler_lays.append([min0, range0])\n",
    "      use_lays.append(ic)\n",
    "  if len(use_lays)<nlay_feats:\n",
    "    print('removing constant layer features:', nlay_feats-len(use_lays))\n",
    "    dat_lays = dat_lays[:,:,use_lays]\n",
    "    nlay_feats = len(use_lays)\n",
    "      \n",
    "\n",
    "  scaler_surf = []\n",
    "  use_surf = []\n",
    "  for ic in range(nsurf_feats):\n",
    "    min0 = numpy.min(dat_surf[:,ic])\n",
    "    range0 = numpy.max(dat_surf[:,ic]) - min0\n",
    "    if range0 > 0.:\n",
    "      dat_surf[:,ic] = (dat_surf[:,ic] - min0)/range0\n",
    "      scaler_surf.append([min0, range0])\n",
    "      use_surf.append(ic)\n",
    "  if len(use_surf)<nsurf_feats:\n",
    "    print('removing constant surf features:', nsurf_feats-len(use_surf))\n",
    "    dat_surf = dat_surf[:,:,use_surf]\n",
    "    nsurf_feats = len(use_surf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5a3c17-2dba-433c-8b90-98c1a532e9a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ntrg_samps = trg.shape[0]\n",
    "\n",
    "if target=='nflx' or target=='ndiv':\n",
    "  nouts=1\n",
    "  ntrg_levs = trg.shape[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86089c56-6f6e-44c0-98ee-96f635548d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_mlp(nlays, nlay_feats):\n",
    "    profile_input = tensorflow.keras.layers.Input(shape=(nlays, nlay_feats), name='profile_input')\n",
    "    surf_input = tensorflow.keras.layers.Input(shape=(nsurf_feats,), name='surf_input')\n",
    "    flat_profs = tensorflow.keras.layers.Flatten()(profile_input)\n",
    "    raw_in = tensorflow.keras.layers.concatenate([flat_profs, surf_input])\n",
    "    raw_size = (nlays*nlay_feats)+nsurf_feats\n",
    "    prof_size = nlays*nlay_feats\n",
    "\n",
    "    x = tensorflow.keras.layers.Dense(512, use_bias=False, activation='relu')(raw_in)\n",
    "    x = tensorflow.keras.layers.Dense(512, use_bias=False, activation='relu')(x)\n",
    "    x = tensorflow.keras.layers.Dense(256, use_bias=False, activation='relu')(x)\n",
    "    x = tensorflow.keras.layers.Dense(256, use_bias=False, activation='relu')(x)\n",
    "    x = tensorflow.keras.layers.Dense(128, use_bias=False, activation='relu')(x)\n",
    "    x = tensorflow.keras.layers.Dense(128, use_bias=False, activation='relu')(x)\n",
    "\n",
    "    main_output = tensorflow.keras.layers.Dense(ntrg_levs, use_bias=True, activation='linear', name='main_output')(x)\n",
    "    model = tensorflow.keras.models.Model(inputs=[profile_input, surf_input], outputs=[main_output])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e0d400-b092-40a5-bc2e-4b05354af9ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_cnn(nlays, nlay_feats):\n",
    "    profile_input = tensorflow.keras.layers.Input(shape=(nlays, nlay_feats), name='profile_input')\n",
    "    surf_input = tensorflow.keras.layers.Input(shape=(nsurf_feats,), name='surf_input')\n",
    "    flat_profs = tensorflow.keras.layers.Flatten()(profile_input)\n",
    "    raw_in = tensorflow.keras.layers.concatenate([flat_profs, surf_input])\n",
    "    raw_size = (nlays*nlay_feats)+nsurf_feats\n",
    "    prof_size = nlays*nlay_feats\n",
    "\n",
    "    out = tensorflow.keras.layers.ZeroPadding1D(padding=1)(profile_input)\n",
    "    out = tensorflow.keras.layers.Conv1D(32, 3, strides=1, activation='relu', use_bias=False, kernel_initializer='glorot_uniform', bias_initializer='zeros')(out)\n",
    "    ident = out\n",
    "    out = tensorflow.keras.layers.ZeroPadding1D(padding=1)(out)\n",
    "    out = tensorflow.keras.layers.Conv1D(32, 3, strides=1, activation='relu', use_bias=False, kernel_initializer='glorot_uniform', bias_initializer='zeros')(out)\n",
    "    out = tensorflow.keras.layers.ZeroPadding1D(padding=1)(out)\n",
    "    out = tensorflow.keras.layers.Conv1D(32, 3, strides=1, activation='relu', use_bias=False, kernel_initializer='glorot_uniform', bias_initializer='zeros')(out)\n",
    "    x = tensorflow.keras.layers.add([out, ident])\n",
    "    out = tensorflow.keras.layers.Flatten()(x)\n",
    "    out = tensorflow.keras.layers.Dense(prof_size, use_bias=False, activation='relu')(out)\n",
    "\n",
    "    out = tensorflow.keras.layers.concatenate([out, surf_input])\n",
    "    x = tensorflow.keras.layers.add([out, raw_in])\n",
    "    x = tensorflow.keras.layers.Dense(1024, use_bias=False, activation='relu')(x)\n",
    "    x = tensorflow.keras.layers.Dense(1024, use_bias=False, activation='relu')(x)\n",
    "\n",
    "    main_output = tensorflow.keras.layers.Dense(ntrg_levs, use_bias=True, activation='linear', name='main_output')(x)\n",
    "    model = tensorflow.keras.models.Model(inputs=[profile_input, surf_input], outputs=[main_output])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d1d0098-fdf0-41d9-b876-5c638d7ac330",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict = {'mlp': {'build_func': build_model_mlp,},\n",
    "              'cnn_1d': {'build_func': build_model_cnn,},\n",
    "             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaef9962-ab06-4985-98c3-415b553c59ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for model_name in model_dict.keys():\n",
    "    print(f'building and training model {model_name}')\n",
    "    model_dict[model_name]['model_object'] = model_dict[model_name]['build_func'](nlays=nlays, nlay_feats=nlay_feats)\n",
    "    model_dict[model_name]['model_object'].compile(loss='mean_absolute_error',\n",
    "                                                   optimizer='adam')\n",
    "    model_dict[model_name]['model_object'].fit([dat_lays, dat_surf], trg, epochs=1, batch_size=32, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a6181cd-b35f-40e7-a7ec-1dc59c555aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('loading',fn_dat_lays_test)\n",
    "with numpy.load(data_dir / fn_dat_lays_test) as npzfile:\n",
    "    dat_lays_test = npzfile['dat_lays']\n",
    "\n",
    "print('loading',fn_dat_surf_test)\n",
    "with numpy.load(data_dir / fn_dat_surf_test) as npzfile:\n",
    "    dat_surf_test = npzfile['dat_surf']\n",
    "\n",
    "print('loading',fn_trg_test)\n",
    "with numpy.load(data_dir / fn_trg_test)as npzfile:\n",
    "    if target=='nflx':\n",
    "        trg_test = npzfile['trg_levs']\n",
    "    elif target=='ndiv':\n",
    "         trg_test = npzfile['trg_lays']\n",
    "\n",
    "print('loading',fn_trg_ref)\n",
    "with numpy.load(data_dir / fn_trg_ref) as npzfile:\n",
    "    if target=='nflx':\n",
    "         trg_ref = npzfile['trg_levs_ref'] # GA& operational output\n",
    "    elif target=='ndiv':\n",
    "         trg_ref = npzfile['trg_lays_ref']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f682833d-f442-4659-9d45-1b3ddb8b442f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale test data\n",
    "if scale_data: # normalize by range\n",
    "  dat_lays_test = dat_lays_test[:,:,use_lays]\n",
    "  for ic in range(nlay_feats):\n",
    "    dat_lays_test[:,:,ic] = (dat_lays_test[:,:,ic] - scaler_lays[ic][0])/scaler_lays[ic][1]\n",
    "\n",
    "  dat_surf_test = dat_surf_test[:,use_surf]\n",
    "  for ic in range(nsurf_feats):\n",
    "    dat_surf_test[:,ic] = (dat_surf_test[:,ic] - scaler_surf[ic][0])/scaler_surf[ic][1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da2b685-e62b-4cc3-91b0-47829a29b93e",
   "metadata": {},
   "source": [
    "### Do inference on test data\n",
    "\n",
    "Next we load in the test data and do inference to check the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1825e462-ad13-40c9-88da-c95a9e9ab329",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66db5a77-b8ce-4d8a-b6fd-2925d06a1552",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name, selected_model in model_dict.items():\n",
    "  predictions[model_name] = selected_model['model_object'].predict([dat_lays_test, dat_surf_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59a213f-78a2-42d0-b7af-e095b2c80337",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "metrics_dict = {}\n",
    "for model_name in model_dict.keys():\n",
    "    metrics_dict[model_name] = {}\n",
    "    metrics_dict[model_name]['me_p'] = numpy.zeros(ntrg_levs)\n",
    "    metrics_dict[model_name]['me_ctl'] = numpy.zeros(ntrg_levs)\n",
    "    metrics_dict[model_name]['mae_p'] = numpy.zeros(ntrg_levs)\n",
    "    metrics_dict[model_name]['mae_ctl'] = numpy.zeros(ntrg_levs)\n",
    "    for ilev in range(ntrg_levs):\n",
    "      metrics_dict[model_name]['me_p'][ilev] = numpy.mean(predictions[model_name][:,ilev] - trg_test[:,ilev])\n",
    "      metrics_dict[model_name]['me_ctl'][ilev] = numpy.mean(trg_ref[:,ilev] - trg_test[:,ilev])\n",
    "      metrics_dict[model_name]['mae_p'][ilev] = numpy.mean(numpy.abs(predictions[model_name][:,ilev] - trg_test[:,ilev]))\n",
    "      metrics_dict[model_name]['mae_ctl'][ilev] = numpy.mean(numpy.abs(trg_ref[:,ilev] - trg_test[:,ilev]))    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b203ea6-1c7a-4297-9bed-fc6e4fc5c50a",
   "metadata": {},
   "source": [
    "### Visualise metrics\n",
    "\n",
    "Display the performance metrics for our trained algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a763df-2c24-4e69-9a4f-26363efe9708",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759c8a63-1080-4d69-8450-ca35c6dac757",
   "metadata": {},
   "outputs": [],
   "source": [
    "yax = numpy.arange(1,len(metrics_dict['mlp']['me_p'][1:])+1)[::-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b55ed2c-adc9-41a2-9904-29d8c37de044",
   "metadata": {},
   "outputs": [],
   "source": [
    "try more multiple epochs\n",
    "load larger training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e2abc1-9d47-4b92-b998-2a5075f37680",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc0c580-1738-4262-a105-bd62f367a5be",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'mlp'\n",
    "fig1 = matplotlib.pyplot.figure('compare_NN_results', figsize=(16,6))\n",
    "ax1 = fig1.add_subplot(1,2, 1 ,title=f'mean error results for {model_name}')\n",
    "ax1.plot(metrics_dict[model_name]['me_p'][1:],yax, '-r', label='ME emu')\n",
    "ax1.set_xlabel('level')\n",
    "ax1.set_ylabel('flux / flux div. difference')\n",
    "ax1.legend(loc='upper right')\n",
    "\n",
    "ax1.plot(metrics_dict[model_name]['me_ctl'][1:],yax, '-c', label='ME ga7')\n",
    "ax1.set_xlabel('level')\n",
    "ax1.set_ylabel('flux / flux div. difference')\n",
    "ax1.legend(loc='upper right')\n",
    "\n",
    "ax1 = fig1.add_subplot(1,2, 2, title=f'MAE results for {model_name}')\n",
    "ax1.plot(metrics_dict[model_name]['mae_p'][1:],yax, '--r', label='MAE emu')\n",
    "ax1.set_xlabel('level')\n",
    "ax1.set_ylabel('flux / flux div. difference')\n",
    "ax1.legend(loc='upper right')\n",
    "\n",
    "ax1.plot(metrics_dict[model_name]['mae_ctl'][1:],yax, '--c', label='MAE ga7')\n",
    "ax1.set_xlabel('level')\n",
    "ax1.set_ylabel('flux / flux div. difference')\n",
    "ax1.legend(loc='upper right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9de013f-997f-4faf-a8fd-7558213d7d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1 = matplotlib.pyplot.figure('compare_NN_results', figsize=(16,6))\n",
    "for ix1,model_name in enumerate(model_dict.keys()):\n",
    "    ax1 = fig1.add_subplot(2,1, 1 ,title=f'results for {model_name}')\n",
    "    ax1.plot(metrics_dict[model_name]['me_p'][1:],yax, '-r', label='ME emu')\n",
    "    ax1.set_xlabel('level')\n",
    "    ax1.set_ylabel('flux / flux div. difference')\n",
    "    ax1.legend(loc='upper right')\n",
    "  \n",
    "    \n",
    "    # ax1 = fig1.add_subplot(2,4, (4*ix1) + 3, title=f'results for {model_name}')\n",
    "    ax1.plot(metrics_dict[model_name]['me_ctl'][1:],yax, '-c', label='ME ga7')\n",
    "    ax1.set_xlabel('level')\n",
    "    ax1.set_ylabel('flux / flux div. difference')\n",
    "    ax1.legend(loc='upper right')\n",
    "    \n",
    "    # ax1 = fig1.add_subplot(2,4,(4*ix1) + 2, title=f'results for {model_name}')\n",
    "    ax1.plot(metrics_dict[model_name]['mae_p'][1:],yax, '--r', label='MAE emu')\n",
    "    ax1.set_xlabel('level')\n",
    "    ax1.set_ylabel('flux / flux div. difference')\n",
    "    ax1.legend(loc='upper right')\n",
    "    \n",
    "    ax1 = fig1.add_subplot(2,4, (4*ix1) + 4, title=f'results for {model_name}')\n",
    "    # ax1.plot(metrics_dict[model_name]['mae_ctl'][1:],yax, '--c', label='MAE ga7')\n",
    "    ax1.set_xlabel('level')\n",
    "    ax1.set_ylabel('flux / flux div. difference')\n",
    "    ax1.legend(loc='upper right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc9b03dd-126c-429f-a12d-1b34bf0a4447",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c06fbf53-7998-4e67-8743-4f735e03df59",
   "metadata": {},
   "source": [
    "### Example - Recurrent Neural Network\n",
    "\n",
    "** This example is under construction, do not use!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb8c5c2-6998-41a5-873d-4a3e2ca2280a",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    falklands_data_dir = os.environ['OPMET_ROTORS_DATA_ROOT']\n",
    "except KeyError:\n",
    "    falklands_data_dir = '/project/informatics_lab/data_science_cop/ML_challenges/2021_opmet_challenge'\n",
    "falklands_data_dir = pathlib.Path(falklands_data_dir) /  'Rotors'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c50bc3b8-b01f-4caf-87fb-95c23c0188ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "falklands_data_fname = 'new_training.csv'\n",
    "falklands_data_path = falklands_data_dir / falklands_data_fname\n",
    "falklands_df = pandas.read_csv(falklands_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6424020-429b-43d6-813b-e2f266b087cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_feature_names = [f'air_temp_{i1}' for i1 in range(1,23)]\n",
    "humidity_feature_names = [f'sh_{i1}' for i1 in range(1,23)]\n",
    "wind_direction_feature_names = [f'winddir_{i1}' for i1 in range(1,23)]\n",
    "wind_speed_feature_names = [f'windspd_{i1}' for i1 in range(1,23)]\n",
    "target_feature_name = 'rotors_present'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1acb91b-84dd-4c50-a163-d402bd085287",
   "metadata": {},
   "outputs": [],
   "source": [
    "falklands_df = falklands_df.rename({'Rotors 1 is true': target_feature_name},axis=1)\n",
    "falklands_df.loc[falklands_df[falklands_df[target_feature_name].isna()].index, target_feature_name] = 0\n",
    "falklands_df['DTG'] = pandas.to_datetime(falklands_df['DTG'])\n",
    "falklands_df = falklands_df.drop_duplicates(subset=['DTG'])\n",
    "falklands_df = falklands_df[~falklands_df['DTG'].isnull()]\n",
    "falklands_df = falklands_df[(falklands_df['wind_speed_obs'] >= 0.0) &\n",
    "                            (falklands_df['air_temp_obs'] >= 0.0) &\n",
    "                            (falklands_df['wind_direction_obs'] >= 0.0) &\n",
    "                            (falklands_df['dewpoint_obs'] >= 0.0) \n",
    "                           ]\n",
    "falklands_df = falklands_df.drop_duplicates(subset='DTG')\n",
    "falklands_df[target_feature_name]  = falklands_df[target_feature_name] .astype(bool)\n",
    "falklands_df['time'] = pandas.to_datetime(falklands_df['DTG'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2188b0-59b8-40cc-b07a-efab21d83d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_v_wind(wind_dir_name, wind_speed_name, row1):\n",
    "    return math.cos(math.radians(row1[wind_dir_name])) * row1[wind_speed_name]\n",
    "\n",
    "def get_u_wind(wind_dir_name, wind_speed_name, row1):\n",
    "    return math.sin(math.radians(row1[wind_dir_name])) * row1[wind_speed_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b57a26-4c4a-433d-b3c4-b1faa1425173",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "u_feature_template = 'u_wind_{level_ix}'\n",
    "v_feature_template = 'v_wind_{level_ix}'\n",
    "u_wind_feature_names = []\n",
    "v_wind_features_names = []\n",
    "for wsn1, wdn1 in zip(wind_speed_feature_names, wind_direction_feature_names):\n",
    "    level_ix = int( wsn1.split('_')[1])\n",
    "    u_feature = u_feature_template.format(level_ix=level_ix)\n",
    "    u_wind_feature_names += [u_feature]\n",
    "    falklands_df[u_feature] = falklands_df.apply(functools.partial(get_u_wind, wdn1, wsn1), axis='columns')\n",
    "    v_feature = v_feature_template.format(level_ix=level_ix)\n",
    "    v_wind_features_names += [v_feature]\n",
    "    falklands_df[v_feature] = falklands_df.apply(functools.partial(get_v_wind, wdn1, wsn1), axis='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f69909-cb92-49e5-a991-72643d61f2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "rotors_train_df = falklands_df[falklands_df['time'] < datetime.datetime(2020,1,1,0,0)]\n",
    "rotors_test_df = falklands_df[falklands_df['time'] > datetime.datetime(2020,1,1,0,0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "403889d1-adb2-49b3-858c-c089407fb23f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preproc_input(data_subset, pp_dict):\n",
    "    return numpy.concatenate([scaler1.transform(data_subset[[if1]]) for if1,scaler1 in pp_dict.items()],axis=1)\n",
    "\n",
    "def preproc_target(data_subset, enc1):\n",
    "     return enc1.transform(data_subset[[target_feature_name]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f078f090-a21e-4e3f-b036-d1c50405dcda",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_feature_names = temp_feature_names + humidity_feature_names + u_wind_feature_names + v_wind_features_names\n",
    "preproc_dict = {}\n",
    "for if1 in input_feature_names:\n",
    "    scaler1 = sklearn.preprocessing.StandardScaler()\n",
    "    scaler1.fit(rotors_train_df[[if1]])\n",
    "    preproc_dict[if1] = scaler1\n",
    "    \n",
    "target_encoder = sklearn.preprocessing.LabelEncoder()\n",
    "target_encoder.fit(rotors_train_df[[target_feature_name]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb9d77f-3e82-4e58-b5f5-0009003fb881",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_rotors = preproc_input(rotors_train_df, preproc_dict)\n",
    "y_train_rotors = preproc_target(rotors_train_df, target_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa42d00-5000-4496-ae18-e13e2fa4d9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_rotors = preproc_input(rotors_test_df, preproc_dict)\n",
    "y_test_rotors = preproc_target(rotors_test_df, target_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b32b8af-8c64-42a4-8021-be7a4dd9bd21",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_learning_rate=2e-5\n",
    "drop_out_rate=0.2\n",
    "n_epochs=50\n",
    "batch_size=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15cb9821-8811-4591-92fe-e0d37ce0db8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_nodes = 300\n",
    "n_layers = 4\n",
    "inputs_shape=X_train_rotors.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4558c59e-be48-401d-a6ad-07d0a22190f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_ff_nn(n_layers, input_shape):\n",
    "    model = tensorflow.keras.models.Sequential()\n",
    "    model.add(tensorflow.keras.layers.Dropout(drop_out_rate, input_shape=(input_shape,)))\n",
    "    for i in numpy.arange(0,n_layers):\n",
    "        model.add(tensorflow.keras.layers.Dense(n_nodes, activation='relu', kernel_constraint=tensorflow.keras.constraints.max_norm(3)))\n",
    "        model.add(tensorflow.keras.layers.Dropout(drop_out_rate))\n",
    "    model.add(tensorflow.keras.layers.Dense(2, activation='softmax'))             # This is the output layer \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d41e5dc2-a4c1-444f-94ac-28460f5e8841",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "rotors_ff_model = build_ff_nn(n_layers=n_layers, input_shape=inputs_shape)\n",
    "opt = tensorflow.optimizers.Adam(learning_rate=initial_learning_rate)  \n",
    "rotors_ff_model.compile(optimizer=opt, loss='mse', metrics=[tensorflow.keras.metrics.RootMeanSquaredError()])\n",
    "rotors_history=rotors_ff_model.fit(X_train_rotors, \n",
    "                                   y_train_rotors, \n",
    "                                   validation_data=(X_test_rotors, \n",
    "                                                    y_test_rotors), \n",
    "                                   epochs=n_epochs, \n",
    "                                   batch_size=batch_size, \n",
    "                                   shuffle=True,\n",
    "                                   verbose=False,\n",
    "                                  )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc6c527-fbf5-411f-a81b-ee170251c87f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_LSTM_model(input_shape):\n",
    "    model = tensorflow.keras.Sequential()\n",
    "\n",
    "    model.add(tensorflow.keras.layers.Input(shape=(1,input_shape,)))\n",
    "    # Add a LSTM layer with 128 internal units.\n",
    "    model.add(tensorflow.keras.layers.LSTM(64,))\n",
    "\n",
    "    # Add a Dense layer with 10 units.\n",
    "    model.add(tensorflow.keras.layers.Dense(10))\n",
    "    \n",
    "    # add output layer\n",
    "    model.add(tensorflow.keras.layers.Dense(2, activation='softmax'))             # This is the output layer \n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293d4fd9-1f80-4e27-b51c-b38ec8f3c86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train_rotors.reshape([X_train_rotors.shape[0],1,X_train_rotors.shape[1]]).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a81caf-830e-407e-92aa-60cb1e19be61",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "rotors_rnn_model = build_LSTM_model(input_shape=inputs_shape)\n",
    "opt = tensorflow.optimizers.Adam(learning_rate=initial_learning_rate)  \n",
    "rotors_rnn_model.compile(optimizer=opt, loss='mse', metrics=[tensorflow.keras.metrics.RootMeanSquaredError()])\n",
    "rotors_history_rnn = rotors_rnn_model.fit(X_train_rotors.reshape([X_train_rotors.shape[0],1,X_train_rotors.shape[1]]), \n",
    "                                         y_train_rotors, \n",
    "                                         validation_data=(X_test_rotors, \n",
    "                                                          y_test_rotors), \n",
    "                                         epochs=n_epochs, \n",
    "                                         batch_size=batch_size, \n",
    "                                         shuffle=True,         \n",
    "                                         verbose=False,\n",
    "                                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd273796-c8c3-49d7-bcea-94662aebec48",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_rotors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89dc67a7-e517-4d48-83e2-ebdac076cc3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce159e2-df00-4b91-8b2a-37e8f7f141b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7f934c-3d9c-46dc-baea-88942f0600e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d32d8f31-1bce-4f9a-a653-51c9c10fa7a4",
   "metadata": {},
   "source": [
    "## Tutorial - Metrics\n",
    "\n",
    "\n",
    "Description of metrics\n",
    "* classification metrics\n",
    "* regression metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec4d843f-d6c6-4089-9207-826814820407",
   "metadata": {},
   "source": [
    "## Exercise - Metrics\n",
    "xx\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f9ba8b3-f653-497e-a8f3-cf86c30d3b54",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "\n",
    "The information presented in this notebook hopefully helps you make appropriate choices of algorithm and metrics for your machine learning pipeline. This is not an easy or straight forward process, often relying on previous experience and the developed intuition that comes with it. The following articles provide further guidance on this process:\n",
    "* [scikit-learn choosing the right estimator](https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html)\n",
    "* [How to choose the right algorithm - Towards Data Science](https://towardsdatascience.com/do-you-know-how-to-choose-the-right-machine-learning-algorithm-among-7-different-types-295d0b0c7f60)\n",
    "* [Guide to Choosing the right ML Algorithm](https://medium.com/dataseries/an-easy-guide-to-choose-the-right-machine-learning-algorithm-for-your-task-b0f6d77aab75)\n",
    "* [How to choose a ML algorithm - Some Guidelines](https://www.datasciencecentral.com/how-to-choose-a-machine-learning-model-some-guidelines/) \n",
    "* [ML Algorithm to Use - SAS](https://blogs.sas.com/content/subconsciousmusings/2020/12/09/machine-learning-algorithm-use/)\n",
    "\n",
    "![scikit-learn How to choose an estimator](https://scikit-learn.org/stable/_static/ml_map.png)\n",
    "\n",
    "![Choosing an ML algorithm](https://blogs.sas.com/content/subconsciousmusings/files/2017/04/machine-learning-cheet-sheet-2.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb9d6187-9d49-4722-b56b-77e05b1989b5",
   "metadata": {},
   "source": [
    "## Dataset Info\n",
    "\n",
    "### XBT Data\n",
    "This data is a preprocessed version of XBT profile data which is freely available from the [World Ocean Database](https://www.ncei.noaa.gov/products/world-ocean-database).\n",
    "More information on the XBT data is available from the [WOD documentation](http://www.ncei.noaa.gov/sites/default/files/2020-04/wod_intro_0.pdf) from NOAA.\n",
    "\n",
    "### ML Emulation of Socrates Radiation Scheme\n",
    "Crown Copyright 2021 - This data was produced by Tom Dunstan as part of a project to emulate the [Socrates radiation scheme](https://code.metoffice.gov.uk/trac/socrates) used in the Unified Model, using machine learning. The data is the inputs and output from the UM radiation scheme as used for the emulation project.\n",
    "\n",
    "### Falklands Rotors Challenge Dataset\n",
    "Crown Copyright 2021 - This dataset was created by Met Office Chief Operational Meteorologist Steve Ramsdale from Met Office forecast and observation data.\n",
    "* Model Data - Met Office Global 10km resolution model\n",
    "* Observations - made by meteorologists at Mount Pleasant airfield in the Falkland Islands.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f832efc6-7cb1-47d3-a7f2-89655afa20e8",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "### Decision trees\n",
    "* [Introduction to decision trees - Masters in Data Science](https://www.mastersindatascience.org/learning/introduction-to-machine-learning-algorithms/decision-tree/#:~:text=A)\n",
    "* [Decision Trees Explained - Towards Data Science](https://towardsdatascience.com/decision-trees-explained-3ec41632ceb6)\n",
    "* Random Forests\n",
    "* [Mathematics of Random Forests](https://towardsdatascience.com/the-mathematics-of-decision-trees-random-forest-and-feature-importance-in-scikit-learn-and-spark-f2861df67e3)\n",
    "\n",
    "### Neural Networks\n",
    "* [Introduction to Neural Networks - Kaggle](https://www.kaggle.com/code/carlosaguayo/introduction-to-neural-networks/notebook)\n",
    "* [Back propagation - wikipedia](https://en.wikipedia.org/wiki/Backpropagation)\n",
    "* [Back propagation - brilliant wiki](https://brilliant.org/wiki/backpropagation/#:~:text=Backpropagation%2C%20short%20for%20%22backward%20propagation,to%20the%20neural%20network's%20weights)\n",
    "* [Detailed explanation of back propagation](http://neuralnetworksanddeeplearning.com/chap2.html)\n",
    "* [Introduction to Deep Learning - Kaggle](https://www.kaggle.com/learn/intro-to-deep-learning)\n",
    "* [Introduction to Neural Networks - IBM](https://www.ibm.com/cloud/learn/neural-networks)\n",
    "* [Neural Networks - MIT](https://news.mit.edu/2017/explained-neural-networks-deep-learning-0414)\n",
    "* [Build an RNN in Keras](https://pythonalgos.com/build-a-simple-recurrent-neural-network-with-keras/)\n",
    "* [RNN Kaggle Tutorial](https://victorzhou.com/blog/keras-rnn-tutorial/)\n",
    "* [What is an LSTM cell?](https://tung2389.github.io/coding-note/unitslstm)\n",
    "* [Intro to Convolutional Neural Networks (CNNs)](https://towardsdatascience.com/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53)\n",
    "\n",
    "### Metrics\n",
    "* [Regression and Classification metrics - scikit-learn](https://scikit-learn.org/stable/modules/model_evaluation.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".conda-ml-weather-tutorial-tf Python (Conda)",
   "language": "python",
   "name": "conda-env-.conda-ml-weather-tutorial-tf-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
